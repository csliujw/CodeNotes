# Few Shot 相关

[(1条消息) 小样本语义分割_DeepWWJ的博客-CSDN博客_小样本语义分割](https://blog.csdn.net/qq_21157073/article/details/116044143)

<b>元学习</b>：模型具有很强的迁移能力，只需要少量的样本就可以完成新类别的识别。

<b>度量学习</b>：由参数引导的分类器转化为由距离指导的分类器，只要模型具有很强的距离映射能力，那么将很好的解决小样本学习。

<b>few shot ≈ 元学习+度量学习</b>

小样本语义分割语义分割的重点就是原型 P 表达的准确性，可以粗鲁的认为只要 P 表达准确，那么小样本学习也就差不多完成了，其它的操作只是在为 P 服务。

<b>单纯的利用 labeled target 生成的原型不具备很强的代表性，要想办法利用那些 unlabeled target 的数据。</b>

## 交叉support-set和query-set

<b>PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</b>

使用 support-prototype 来搜索 query-set 中的图片，参考搜索的结果，选置信度大的 query-set 中的特征，计算得到 query-prototype，再使用 query-prototype 来搜索 support-set，将搜索得到的结果计算损失。

感觉还是从正则化上考虑的。将 unlabeled 中学得的原型去预测 labeled 中的图片，然后用 labeled 中的图片的 label 去纠正错误的内容。

## 多原型匹配

单一的原型受限于样本个数，并不能很好的对类别进行特征表达；同时单纯的取均值得到的原型也太过于粗鲁。所以，建议使用多个原型来对类别进行表达，这样一来，不同原型可以关注图片中类别的不同区域，同时摒弃掉直接取均值的策略，采用更加合理的自动适配来进行原型的提取。

既然样本受限，学习多个原型虽然可以提升特征的表达，但是正如上面说的，还是会受限于样本。<span style="color:red">为什么不充分利用 ublabeled 的数据呢？</span>

>Adaptive Prototype Learning and Allocation for Few-Shot Segmentation（2021 CVPR）

图片中局部原型不使用取均值得到；多个原型关注不同区域；聚类原型。

## 问题

- Mean teacher 模型，用 mean teacher 计算原型，可以避免原型的变动过大，进而减小拟合的难度。
- 利用好 labeled 的原型，再用 unlabeled 的原型做一个微调。这个微调的过程中难免会让原型朝错误的方向更新

# 推荐论文

<a href="https://arxiv.org/pdf/1908.06391v2.pdf">PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</a>

<a href="https://arxiv.org/pdf/2008.03898v2.pdf">Prototype Mixture Models for Few-shot Semantic Segmentation</a>

<a href="https://arxiv.org/pdf/2203.07615v2.pdf">Learning What Not to Segment: A New Perspective on Few-Shot Segmentation</a>

# PANet

先不加自己的东西看看，看能跑到什么精度。然后加加入自己的东西，看可以跑出什么样的结果。

## Introduce

<a href="https://arxiv.org/pdf/1908.06391v2.pdf">PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</a>

本文从度量学习的角度解决了具有挑战性的少镜头分割问题，并提出了一种新的原型对齐网络 PANet，以更好地利用支持集（Support Set）的信息。PANet 从嵌入空间（embedding space）内的一些支持图像中学习特定于类的原型表示，然后通过将每个像素与学习到的原型进行匹配来对查询图像进行分割。PANet 提供了高质量的原型，它代表了每个语义类，同时对不同的类有区别。此外，<span style="color:orange">PANet 还引入了支持和查询之间的原型对齐正则化。</span>因此，PANet 充分利用了来自支持的知识，并对少镜头分割提供了更好的泛化。

现有的少镜头分割方法通常从少量的支持图像中学习，然后将学习到的知识输入到一个参数模块中进行分割查询。然而，这种方案有两个缺点，因此推广效果并不令人满意。首先，它们没有区分知识提取和分割过程，这可能是一个问题，因为分割模型的 representation 与支持的语义特征混合。因此，我们建议分离这两部分作为原型提取和非参数度量学习。原型被优化为每个语义类的紧凑和鲁棒表示，非参数度量学习在嵌入空间内通过像素级匹配进行分割。此外，我们不像以前的方法那样只使用支持的注释，而是充分利用了查询集的数据。<span style="color:orange">为此，我们引入了一种新的原型对齐正则化，通过在反向方向上执行 few shot 分割。即，查询图像及其预测的掩码被视为一个新的支持集，用于对之前的支持图像进行分割。通过这种方式，我们鼓励模型在支持和查询之间生成更一致的原型，从而提供更好的泛化性能。</span>

因此，我们开发了一个原型对齐网络 (PANet) 来处理少镜头分割，如图1所示。PANet 首先通过一个共享的特征提取器将不同的前景对象和背景嵌入到不同的原型中。这样，每个学习到的原型都代表了相应的类，同时与其他类有充分的区别。然后，通过引用最接近其嵌入表示的类特定原型，对查询图像的每个像素进行标记。我们发现，即使每个类只有一个支持图像，PANet 也可以提供令人满意的分割结果，优于最先进的技术。此外，通过与查询图像及其预测掩码形成一个新的支持集，并对原始支持集进行分割，实现了原型对齐正则化。我们发现，这确实鼓励从查询生成的原型与支持支持良好地对齐。注意，模型只在训练中进行了正则化，查询图像不应该与测试图像混淆。

这种结构设计有几个优点。首先，它没有引入额外的可学习参数，因此不太容易发生过拟合。其次，在 PANet 中，原型的嵌入和预测是对计算出的特征图进行的，因此分割不需要额外的通过网络。此外，由于正则化只在训练中进行，因此推理的计算成本不会增加。

- 提出了一种简单而有效的少镜头分割面板。该模型利用了原型上的度量学习，这不同于大多数采用参数分类架构的现有工作。
- 提出了一种新的原型对齐正则化，以充分利用支持知识来提高少镜头学习。
- 该模型可以直接应用于从一些具有弱注释的例子中学习
- 成为了新的 sota

提到了一个很有意思的做法，把 support 中与 query 相同类别的高层特征与 query 的特征进行融合，让这个高层特征去指导 query 的特征。和那个 FD 有些像。

## Method

### Method overview

和常规的原型一样，

1️⃣用 mask 全局平均池化从 support 中得到原型，

2️⃣通过将每个像素标记为最近的原型的类，对查询图像进行分割。

3️⃣引入的一种新的原型对齐正则化（PAR）应用于学习过程中，以鼓励模型学习一致的嵌入原型，以进行支持和查询。

### Prototype learning

我们的模型基于原型网络学习每个语义类（包括背景）的代表性和分离良好的原型表示。PANet 没有对整个输入图像[23]进行平均，而是利用支持图像上的掩码注释来分别学习前景和背景的原型。在本工作中，我们采用了晚期融合策略，因为它保持了共享特征提取器的输入一致性。（<span style="color:orange">为什么要算个背景原型呢</span>）

用每个像素和各个原型的距离来决定，它到底属于那个类别。对距离做一个 softmax，将距离转化为概率值。常用的距离有余弦距离和欧式距离，余弦距离乘以一个系数（fixed 20）性能与欧式距离相当，但是余弦距离更容易优化（可能是因为余弦距离是有界的）

计算出 query 的

### Prototype alignment regularization

更好地利用支持信息来指导 few shot 的学习过程，并从几个例子中帮助增强结果模型的通用性。

如果模型能够使用从支持图像中提取的原型来预测查询的良好分割掩码，那么基于预测的掩码从查询集中学习到的原型应该能够很好地分割支持图像<span style="color:orange">（因为他们的原型是趋于一致的）</span>

因此，PAR 鼓励所得到的分割模型进行反向的 few shot learning，即以 query 和预测的掩码作为 new support 来学习对 original support 的分割。这在 support prototype 和 query 之间施加了相互对齐，并从支持中学习更丰富的知识。请注意，这里所有的支持和查询图像都来自训练集。

在获得查询图像的分割预测后，我们对查询特征进行相应的掩码平均池化，得到另一组原型 P'。接下来，我们使用非参数方法来预测 support 图像的分割掩模，将预测与地面真实注释进行比较，以计算损失$L_{PAR}$。PAR 的整个过程可以看作是交换支持集和查询集。
$$
L_{total} = L_{seg}+\lambda L_{PAR}
$$
其中 λ 作为正则化强度，λ=0 降低为没有 PAR 的模型。在我们的实验中，我们保持 λ 为 1，因为不同的值几乎没有什么改善。算法总结了少镜头分割的整个训练和测试过程。

# Semi-supervised Domain Adaptive Structure Learning

## Abstract

摘要-半监督域自适应(SSDA)是一个相当具有挑战性的问题，需要用克服两个问题：

1)对注释不佳的数据的过拟合和

2)跨领域的分布转移。

<span style="color:orange">但是，领域自适应(DA)和半监督学习(SSL)方法的简单组合往往不能解决这两个问题。</span>在本文中，我们引入了一种自适应结构学习方法来规范 SSL 和 DA 的协作。受多视图学习的启发，<span style="color:orange">我们提出的框架由一个共享的特征编码器网络和两个分类器网络组成，为相互矛盾的目的进行训练</span>。

- 其中一种分类器用于对目标特征进行分组，以提高类内密度，扩大了分类聚类对鲁棒表示学习的差距。
- 另一个分类器，作为一个正则化器，试图分散源特征，以提高决策边界的平滑性。

目标聚类和源扩展的迭代使目标特征很好地封闭在群的扩张边界内
对于跨域特征对齐和部分标记数据学习的联合地址，我们采用最大平均偏差(MMD)距离最小化和自训练(ST)将矛盾结构投影到共享视图中，以做出可靠的最终决策。在标准的SSDA基准测试上的实验结果，包括 DomainNet 和 Office-home，证明了我们的方法比最先进的方法准确性和鲁棒性更强。

## Introduction

本文是对 UODA 的改进

跨域数据/特征之间的相当大的分布不匹配（即领域差距）使大多数建立在分布共享假设基础上的传统机器学习模型退化。特征对齐方法试图通过最小化投影在共享特征空间中的特征的某些散度或距离来显式匹配跨域特征。最小化特征差异的关键在于如何定义特征的距离/分歧？

MME[6]和DIRT-T[14]采用熵损失最小化的方法对目标域特征进行条件对齐，假设聚类良好的特征更具鉴别性。但是由于跨域标记数据的不平等，该模型仍然强烈偏向于源域，不可避免地导致两个负面影响：

- 决策边界很容易错误地跨越高密度目标特征的区域
- 靠近决策边界的目标域特征在对齐时很可能被驱动到错误的区域

受弱/半监督学习中常用的噪声/软标签的启发，假设决策边界可以被具有轻微干扰和分散模式的正则化器去偏是合乎逻辑的。在这方面，UODA[15]总结了SSDA的理想 representation 应该包括相互冲突的方面：

- well-clustered target features
- scattered source features。

因此，本文进一步扩展了这项工作，作为一个更强大的自适应领域自适应结构学习框架，借助一个生成器和两个分类器网络的不同目的。

然而，UODA[15] 有两个主要的限制。<b style="color:red">首先，它忽略了跨域特征的显式对齐，这提供了一个强大的约束来避免错误对齐。其次，它没有充分利用不同数据转换之间的一致性，这是 SSL 中普遍采用的，具有学习更多代表性特征的巨大潜力。</b>本文试图扩展 UODA[15 ]来解决这两个挑战。

利用 MMD 损失最小化来解决跨域特征的显示对齐。这种在再生核希尔伯特空间(RKHS)中的显式对齐进一步支持了精确条件对齐的结构学习。

利用自训练，通过增强弱转换数据和强转换数据之间的一致性来自动实现伪标签。这种数据转换自然地带来了实例级正则化的自我监督。未标记数据的最终分数由两个视图/分类器联合推断出来，它们为鲁棒决策提供了互补的信息。

总的来说，本文的贡献有三点：

- 基于矛盾结构学习的隐式特征对齐可能会由于条件不匹配而带来负迁移。本文引入了显式对齐损失，以帮助隐式对齐损失实现更精确的对齐
- 本文不再关注对立结构的学习，而是更加关注重新统一这些矛盾，这也是至关重要的。为了实现这一点，我们采用自我训练来加强不同增强数据的预测之间的一致性，以及相反的结构学习之间的一致性。
- 在流行的基准测试上进行的大量实验，证明了我们提出的方法相对于直接基线[15]和其他最新方法的优势

## Related Works

在传统的机器学习场景中，训练集和测试集应该共享相同的特征分布。这是大多数算法的基本假设。然而，随着机器学习技术扩展到现实世界的应用程序，如大流行预测[18]，并探索不同的数据格式，这个假设不再总是事实。无监督域自适应(UDA)的目标是在不受监督的情况下采用从源域到目标域的模型。具体来说，来自目标样本的任何标签都是不可访问的。因此，与半监督场景相比，这是一个更具挑战性的任务。近年来，有多种[19]、[20]、[21]、[22]、[23]、[24]、[25]方法被提出，并取得了良好的效果。这些方法可以分为三类，

- 1)基于源-目标散度度量的方法，
- 2)基于泛化扩展的方法，
- 3)基于常数项的方法[26]。

第一类的主要策略是将源样本和目标样本投影到一个潜在的公共空间中，在那里这些样本可以通过最小化散度来很好地对齐。

度量学习[27]，[28]曾经是UDA的流行解决方案。随着深度神经网络在最近的体面中取得了相当大的进展，它们自然在UDA中被探索以获得表示。简单的策略是将源特征和目标特征投射到一个共同的子空间中，最小化不同域的 divergence/shitf。[29]设计了一种相关学习策略，有效地调整不同领域的一致性。最大平均差异被设置为UDA[12]的度量标准。虽然这些方法取得了很好的结果，但是距离度量是固定的，不能很好地处理来自不同领域的各种表示。对抗性学习提供了另一种策略来评估域位移，其中部署了鉴别器和编码器，以有效地获得域不变表示(即梯度反转[20]，ADDA[30])。具体来说，CyCADA[31]是ADDA的一个扩展，具有周期一致性/翻译[32]，可以生成由目标样式和源内容组成的合成图像。

虽然对抗性学习在减少域转移方面是有效的，但条件分布不匹配仍然是一个未解决的问题。在 Tri-training[33] 中提出了多分类器框架，它从多个视图推导出伪标签。<span style="color:red">DIRT-T[14]在特征空间和标签空间中都充分地探索了目标特征。探讨了类内知识的密度和类间知识的差异，以调整分类边界。</span>MCD[34]提出了另一种解决方案，即部署两个分类器来学习两个不同的边界，其中边界的差异被用于对抗性学习机制，以获得域不变表示。然而，由于目标标签的难以获取性，很难精确地描述目标特征的条件分布。因此，UDA方法在现实应用中的潜力有限.

半监督领域自适应

半监督域自适应(SSDA)假设目标域中的少数样本被标记，并且在训练阶段可以获得相应的标签。因此，它也被称为为少镜头域适应(FSDA)场景。<span style="color:red">SSDA在现实应用中是一个更实际的设置，因为标记少量样本是成本效益的，它可以导致相当大的性能改进。</span>此外，通过对监督/标签信息的探索，可以以更细粒度的方式充分探索目标领域的分布知识，从而有可能提高领域的自适应性能。然而，这里也有一些缺点。具体来说，<span style="color:red">很难自动平衡源域和目标域之间的学习贡献，这可能会导致目标域的过拟合问题，降低泛化能力和整体测试性能。</span>

有一些工作被提出来解决这些挑战。[35]探索了标签的相关性，并将学习到的语义知识转移到源域和目标领域。该方法采用了软标签评分和匹配损失。CCSA[36]有效地从目标样本中探索潜在的标签相关知识，以进行条件分布匹配。通过分离损失的最大化方式，共同优化了源域和目标域的特征和标签一致性。FADA[37]是CCSA方法的一个扩展。具体地说，设计了一个额外的对抗性学习模块，其中包括多个鉴别器和一个生成器。目标是进一步稳定和调整在不同类中学习的域不变表示。MME[6]共同考虑了源样本和目标样本的预测，并最大化了跨域的预测熵。此外，聚类策略获得了预测置信度，并为编码器训练获得了更多的细粒度熵损失。然而，由于源域和目标域之间的样本数相当不平衡，在目标域中仍然难以控制学习到的决策边界。UODA[15]被提出通过相反的结构学习来解决上述挑战。然而，考虑到标记的目标样本数量相对较少，不可预测的样本分布（如密度和发散度）可能会进一步混淆目标域中的分类器。CDAC[38]是一种采用自适应对抗性聚类和伪标记的最新技术。本文进一步改进了基于两个分类器/视图的CDAC，为鲁棒决策提供了互补的信息。此外，结合隐式结构学习，也可以通过显式对齐来纠正UODA中分类器的混淆。

## Proposed Approach

由于目标域有标签数据和无标签数据的样本数量差距很大，单纯通过有标签数据来训练模型，效果可能不好。

我们的模型由三部分组成：

-  siamese feature extractor networks
- the source-scattering classififier：设计以干扰源特征作为一种结构正则化
- the target-clustering classififier：对 target 特征进行分组，并希望被源域特征包裹（enclose）。

![image-20220616011208468](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220616011208468.png)

两种分类器之间的差异对模型训练起着重要的作用。因此，为了使F1≠F2，我们对监督损失 Lsrc 和 Ltar 应用不同的权重。让一个分类器侧重于学习源域的特征，一个分类器侧重于学习目标域的特征。

> Features Alignment

为了弥补域间的差距，本文提出了显式和隐式的特征对齐方法。对于显式对齐，我们直接最小化跨域特征上的MMD损失，学习相反的特征结构可以看作是隐式对齐。

隐式对齐：这两种分类器具有不同的目的，以学习期望具有被稀疏源特征包围的密集目标特征的易适应结构。对输出softmax分数的条件熵的最小化加强了对特征聚类的高自信的预测，为了实现这一目标，本文应用熵损失来度量特征的接近度。因此，在未标记的目标域上的条件熵可以表示为：P2 表示 x^u 作为类别 k 的概率。

![image-20220616011827931](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220616011827931.png)

特征散射可以简单地看作是特征聚类的反面。为此，我们还将采用条件熵损失Hsrc来实现源特征扩展

![image-20220616011931995](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220616011931995.png)

其中 Hsrc 将以相反的方式优化为Htar来学习相反的结构。

**Explicit Alignment**：用 MMD 作为显示对齐。

 **Views-consistent Self-training**：用来解决过拟合。采用了弱增强、强增强的数据的自训练策略，用于正则化，使决策边界更平滑。

弱增强包括随机水平翻转和随机作物，强增强从10个选择中随机选择两个变换。硬伪标签是由所有类中置信度最高的弱增强数据获得的，是通过两个分类器取均值得到的。

![image-20220616012511495](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220616012511495-16553139121951.png)

**Overall Training：**需要优化三个，backbone，F1（源域发散），F2（目标域聚类）

![image-20220616012955898](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220616012955898.png)

![image-20220616013139135](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220616013139135.png)

其中，λ和β为损失权重，以平衡条件熵损失和监督损失的影响。条件熵损失的最小化使特征从决策边界转向隐式聚类特征。自我训练损失有助于重新统一对立，学习对增强数据的一致预测，这作为鲁棒决策的视图一致正则化



# Incremental Few-Shot Instance Segmentation

这篇论文感觉写的很绕。

整体思想就是，用改进了 TAF，其实就是加了一个 mask predictor 头。然后把它用 TAF 的方式进行训练，得到了一个比较漂亮的结果。训练方式和 TAF 也是类似的，只是多了一个 mask predictor 进行微调。最后那个增量学习，感觉也是个噱头。其实就是用一些新类，提取它们的特征，将它们的特征均值作为 cosine 计算的权重 wi，然后预测的时候计算每个物体和 consine 的相似度，看它和谁像。

> 增量学习科普

<a href="https://zhuanlan.zhihu.com/p/353273834">增量学习综述，值得一看</a>

<b>「增量学习的能力就是能够不断地处理现实世界中连续的信息流，在吸收新知识的同时保留甚至整合、优化旧知识的能力。」</b>

模型在训练的时候，如果将一个预训练模型在其他数据集上进行微调，新的模型在旧数据上的表现会比之前差。这种我们称为灾难性遗忘。造成灾难性遗忘的主要原因是：<b>「传统模型假设数据分布是固定或平稳的，训练样本是独立同分布的」</b>所以模型可以一遍又一遍地看到所有任务相同的数据，但当数据变为连续的数据流时，训练数据的分布就是非平稳的，模型从非平稳的数据分布中持续不断地获取知识时，新知识会干扰旧知识，从而导致模型性能的快速下降，甚至完全覆盖或遗忘以前学习到的旧知识。

解决灾难性遗忘最简单粗暴的方案就是使用所有已知的数据重新训练网络参数，以适应数据分布随时间的变化。但这种方法效率极其低下。而增量学习的主要目标就是在计算和存储资源有限的条件下，在稳定性-可塑性困境中寻找效用最大的平衡点。

> 增量小样本分割

<a href="https://zhuanlan.zhihu.com/p/429277556">增量小样本分割</a>

文中提到的，增加新类不需要重新微调，只需要提供新类的 embeding 即可，避免了繁琐的 retraining 流程。（这步，将新类的 embeding 的均值作为一个特征向量，预测时计算每个类别和所有特征向量的相似度，对对象进行分类。）

## Abstract

内存密集型：要求在训练和测试时间提供每个类的示例。

## Introduction

对实例分割这类任务来说，获取像素级注释是昂贵的。而 few shot 解决了用有限的可用数据进行学习的问题。虽然已经引入的 few solutions 方案显示出了很大的前景，但在实用性和准确性方面还有改进的空间。通常，训练一个新的模型需要使用新类和基类样本的长训练。当我们向一个训练过的网络中添加新类时，也需要重新训练。这是十分耗时的。在增量的少镜头学习中，新类的添加独立于以前的数据，因此减少了计算时间。

本文介绍了第一种增量的少镜头实例分割方法：iMTFA（图1）。本文采用了一种基于 MaskR-CNN 的两阶段训练和微调方法。第一阶段是训练 MaskR-CNN 网络。在第二阶段，重用（re-purposed） RoI level 的全连接层。本质上，将一个固定的特征提取器转换为一个实例特征提取器(IFE)，它会产生与每个类的代表对齐的区别性嵌入。这些嵌入随后被用作余弦相似度分类器中的权重。

论文中提出的方法有几个优点：

- 首先，它不用对新 class 进行昂贵的 retraining，因为这些 class 可以逐步增加。IFE 生成用作类代表的嵌入，而不需要 access base classes。因为我们以一种类不可知的方式来预测定位和分割，所以这些嵌入是添加新类所需要的全部内容
- 其次，与相关方法[8,39]相比，我们的掩模预测器是类别不可知的。与[21]类似，添加新类不需要掩码标签。
- 第三，我们的方法在测试时没有出现性能缺陷。我们既不需要为每个类示例[8,39]提供额外的内存，也不要求逐个传递这些示例（例如，[21]）。

本文的主要贡献有两点（提出的方法性能很好，将现有的非增量方法扩展到 instance seg 上 作为对比）

- 我们提出了第一种增量的少镜头实例分割方法：iMTFA。我们的方法优于目前最先进的 FSIS 和当前最先进的增量 FSOD。
- 为了比较增量方法和非增量方法，我们将现有的 FSOD 方法[36]扩展到实例分割任务 (MTFA)，并展示了最先进的结果。

## Related Work

介绍 instance seg 和 few shot。

### few-shot learning

少镜头学习使模型能够适应只有很少的训练数据可用的新类。通常，使用情景方法[35]，通过提供可分为N个类的查询项和包含N个类的训练示例的支持集。few shot 学习方法很大程度上分为基于优化的[1,9,26]和度量学习[5,10,14,33,34,35]。

> 基于优化的方法

使用以前的任务作为经验，并被训练来产生一个学习者。

> 度量学习

度量学习方法学习一个 feature embedding，使来自同一类的对象在 embedding space 空间中很接近，而不同类的对象距离很远。Koch等人采用了 Siamese 网络，如果查询和支持图像嵌入是同一类的，则距离最小，否则最大化。Matching 网络[35]计算每个学习到的查询和支持嵌入之间的距离，而原型网络[33]计算每个类的代表。关系网络[34]同时学习距离函数和嵌入。与以前只关注新类性能的方法相比，Gidaris 和 Komodakis [10]专注于使用 softmax cosine-similarity 分类器和新类的权重生成器共同对 novel classes 和 base classes 进行分类。最近，Chen等人[5]表明，之前被忽略的 novel classes 的微调通常比情景训练表现更好。最后，Qi等人[25]提出了通过在现有的权重矩阵中添加新的类嵌入的权重印记（weight-imprinting），允许在没用训练的情况下增加新类（即便训练的时候没有新类也没事，依旧可以正确预测新类）

> few shot object detection

few shot 目标检测将 few shot 学习扩展到目标检测。RepMet[30]训练一个度量学习子网络来 encode support set，而 Kang 等人的[13]则在 YOLOv2[27] 之上直接训练一个元学习者。受[5]的启发，Wang 等人开发了TFA[36]，该方法通过两阶段的方法实现了最先进的目标检测技术。TFA 没有对整个网络进行微调，而是首先在基类上训练 Faster R-CNN[28]，然后只对 predictor heads 进行微调。<span style="color:red">（如果类别差异很大，这种微调也会降低在旧数据上的精度）</span>

> few shot instance segmentation

few shot 的实例分割。少部分工作涉及到 FSIS[8,21,39]。大多数方法都为 MaskR-CNN 架构的某些部分提供指导（修改 Mask RCNN 的某些模块），以确保网络更好地了解新的类。Meta R-CNN[39] 和 Siamese Mask R-CNN[21] 都计算支持集的嵌入，并将其与网络主干产生的特征图相结合（就像之前看过的论文，用 embeding 计算一组特征[距离]，然后和主干的特征进行 cat）。该组合通过不同的操作实现，如减法[21]，将网络集中在特定图像区域，或通过拼接以在某个阶段提供额外的信息。FGN[8]通过类似的操作，使用支持集特征嵌入来指导 RPN、RoI 检测器和掩模上采样层。

> Incremental few-shot object detection

在 ONCE[23] 中已经考虑了增量式 few shot 目标检测，它使用 CenterNet[40] 作为骨干来学习类无关的特征提取器和一个针对新类的全类代码生成器网络。

> Incremental few shot instance segmentation

据我们所知，我们是第一个以增量式 FSIS 为目标的公司。FGN和SiameseMaskR-CNN依赖于在测试时通过每个类的示例，这在考虑许多类时需要大量的内存。MetaR-CNN可以预先计算每个类的注意向量，但需要再训练来处理不同数量的类。相比之下，我们的方法可以增量地添加类，而不需要再训练或需要基类的示例。

## Methodology

### Formulation of few-shot instance segmentation

在 few-shot learning 中，我们有一组基类 $C_{base}$，基类中有大量可用的训练数据。还有一组不相交的 novel classes $C_{novel}$，novel classes 中仅有少量的数据. 我们的目标是训练出一个在 novel classes 中表现良好的模型, 或者是在 $C_{base} \ 和 C_{novel}$ 都表现良好的模型. 

在 few shot classification 中, Vinyals 等人引入了 episodic-training(情景训练) 的方法. 情景训练设置了一系列的情景 $E_{i}=(I^q,S_i)$, $S_i$ 是支持集,包含了来自 $C_{train}=C_{novel} ∪C_{base}$ 的 N 个类别,每个类别 K 个样本,(可以得到泛化更好的模型。这种方法也被扩展到 FSOD（例如[13]）和 FSIS（例如[8,39]），通过将图像中的所有对象视为查询，并对每个图像有一个支持集，而不是每个查询。

FSOD 和 FSIS 扩展了上述的方法，但是它们存在一个问题，我们需要解决的不仅仅是分类还有检测和分割。

### MTFA: A non-incremental baseline approach

我们的非增量基线方法扩展了由 Wan 等人引入的两阶段微调(TFA，[36])目标检测方法。我们首先给出一个 TFA 的概述，然后描述我们的扩展，Mask-TFA(MTFA)，其中包括一个实例分割任务。在第3.3节中，我们将 MTFA 扩展为一种增量式方法。

TFA（图2）使用了 Faster R-CNN[28] 和一个两阶段的训练方案。在第一阶段，网络在基类 $C_{base}$上进行训练。在第二阶段，特征提取器 F 被冻结，只训练预测头。F 由网络主干 B、区域建议网络(RPN) 和 RoI 特征提取器 G 组成。因此，在第二阶段只对 RoI 分类器 C 和 box 回归器 R 进行微调。在包含相同数量的 $C_{base}$ 和 $C_{novive}$ 类示例的数据集上执行微调。

MTFA 我们对 TFA 的扩展类似于 MaskR-CNN 扩展 Faster R-CNN：通过在 RoI 上添加一个掩码预测分支（图2）。因此，MTFA 包括一个分支上采样组件和掩模预测器 m 我们还采用两阶段微调方法首先训练网络基类，然后在一个类别平衡的数据集（每个类别都有 K 个样本）上微调所有预测头 C，R 和 M。

余弦相似性分类器。类似于 TFA 和其他度量学习方法[5,10]，<span style="color:red">本文用余弦相似度分类器来学习更多可判别的每类代表。</span> $w_i$ 怎么算的？预测的时候，计算每个对象和 wi 的相似度，来判断它到底属于那个类别。

### iMTFA：Incremental MTFA

MTFA 的主要缺点是需要添加新的类。第二个微调阶段固定了可以识别的新类的数量。如果再次添加新的类需要再次运行这个阶段，不实用，很繁琐。特定于类的掩码和盒子回归器头还需要以通过微调学习到的权重的形式来适应新的类。在本节中，我们将MTFA扩展到一个增量的方法：iMTFA。为此，我们使模型对类别无感知，并在特征提取器水平上学习判别这些 embeddings（ we make the model class-agnostic and learn discriminative embeddings at the feature extractor level.）。这些嵌入被用作分类头中新的类代表，而不需要进一步的训练。添加新类的体系结构和过程如图 3 所示。

<b>Instance Feature Extractor</b>. TFA 和 MTFA 冻结了特征提取器 F，用冻结的特征提取器生成 discriminative vector embeddings。而是对分类器 C 进行了微调，让学到的类别权值 w 和 F 计算出的 RoI 对齐。<b>而我们用一个 IFE 来提到了 fixed feature extractor</b>

我们的方法的关键思想是为每个实例生成 discriminative embeddings。使用特征均值作为权重 wi。这允许我们直接使用实例嵌入作为类代表，而不需要微调。在训练过程中，本文建议取训练 G（RoI 的特征提取器） 这个特征提取器，以便学到对每个实例产生有区别的嵌入 （discriminative embeddings）。

也是采用 two stage 的训练方式，先按 MTFA 的方式在 $C_{base}$ 训练的 Mask R-CNN。然后 G 、分类器C 和 box regressor R 进行微调，并冻结 backbone 和 RPN。微调阶段和 MTFA 有所不同，微调也是只用 $C_{base}$，结构都一样，只是训练过程不一样。

用余弦相似度分类器把 G 训练成一个 sub-network，这个 sub-network 会生成类特征的嵌入（embeddings）。

<b>Creating class representatives：</b>最终的目标是创建新类的权重向量，该向量可以与基类的权重一起放置，在第二个微调阶段之后保存在 C（分类器） 的权重矩阵 W 中。为了实现这一点，每个包含K个可用的新镜头的图像X都被传递给IFE，为每个镜头产生一个特征嵌入，zi=F(X)i。

<b>Class-agnostic box and mask predictors：</b>iMTFA 不需要 box regressor 和 mask predictor （不需要为 novel 类学习特定的 weight）的类特定权重。 通过简单地平均它们计算出的 embeddings 并将它们放在 classification head's 的权重矩阵W中就可以计算出 novel 类的 box 和 mask。

<b>Inference：</b>因为我们以类不可知的方式来预测 box 和 segmentation，所以我们在测试时只需要 class representative。通过计算 RoI的 embedding 和 class representatives 之间的最小余弦来进行预测。

<b>Relation to other methods：</b>一些相关的方法，在训练和测试的时候都需要出现所有的类，训练的开销比较大；而 iMTFA 不需要，训练的开销小不少。

## Experiment

### Experiment setup

我们的主要评估程序遵循了在FSOD[13,36,39]中建立的惯例。我们对 COCO[19]、VOC2007[7] 和 VOC2012[7] 数据集进行了评估。我们按照[13]中提出的 80 个 COCO 类。与 VOC 相交的 20 个类被设置为新类，其余的 60 个类被设置为基类。将 COCO 的 80k 训练图像和 35k 验证图像的并集用于训练，剩余的∼5k 图像作为测试集。VOC数据集结合了VOC2007和VOC2012，所得到的验证集用于测试。我们评估了每个新类有 K=1,5,10 次 shot 的性能。为了避免样本选取带来的随机性，跑了十次，取均值。我们使用每个类的 K 个随机例子运行所有测试10次，并报告平均结果。我们的少镜头评估程序与[36]相同。

把 iMTFA 和 MTFA 和主流的 FSIS 方法进行了比较（Meta R-CNN，Siamese Mask R-CNN，FGN）。除此之外，还和已知的一些增量目标检测方法进行了对比（FSOD，ONCE）

### Implementation details

在 novel classes 上，MTFA 和 iMTFA 都比现有方法要好。（coco-80 中，将 20 个类别作为 novel，60 个类别作为 base）

![image-20220615224425698](C:\development\note\CVNotes\论文阅读\半监督\img\image-20220615224425698.png)

在 AP50 方面，MTFA 超过了Meta R-CNN，但imTFA略落后。这表明我们可能很难找到物体的粗略位置，但在较高的IoU阈值下表现更好。

## 可以改进的地方

- 首先，iMTFA 在生成新的嵌入时不能适应现有的嵌入。诸如[10,35]所采用的注意机制是一个很有前途的未来发展方向。
- 其次，iMTFA 的类不可知的 localization 和分割组件和 MTFA 的类特定的对应组件相比是次优的。一个明显的改进是从生成的嵌入学习到类特定的盒子回归器和掩码预测器的传递函数
- 与 MTFA 相比，IMTFA 的冻结 box regressor 和 mask predictor 引入了基类偏差。采用指导机制也将缓解这一问题





> 意见2 统计数据的类别分布

统计所有有标签数据中各个类别的像素量。

> 意见4 the limitation of domain adaption

两种设置rural1->urban1和urban2->rural2，改成rural1->rural2和urban2->urban1
