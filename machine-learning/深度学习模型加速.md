# 加速模型推理

## ONNX 

将 PyTorch 模型导出为 ONNX 模型进行推理

- 安装 onnx, onnxruntime,`pip install onnx`  `pip install onnxruntime`(CPU 版本) 
- 导出为 onnx 模型。
- 使用 onnxruntime 运行 onnx 模型。

```python
import torch
from models import get_model_resnet
 
def model_converter():
    weight = torch.load('xxx/best34.pt')
    model = get_model_resnet(net_name="resnet34")
    model.load_state_dict(weight)
    model.eval()
    # 静态输入，数据的shape是固定的，固定数据shape是根据任务来的，这里做的是一个分级任务，我就固定shape了
    dummy_input = torch.randn(1, 3, 720, 720, device='cpu')
    input_names = ['data']
    output_names = ['output']
    torch.onnx.export(model, dummy_input, 'resnet34.onnx', 
                      export_params=True, 
                      verbose=True, 
                      input_names=input_names, 
                      output_names=output_names)

model_converter()
```

使用 ONNX 加载模型进行推理

```python
from time import time
import onnxruntime #  此处用的 cpu 版本
import cv2
import numpy as np
import time

def softmax(x):
    x -= np.max(x, axis= 1, keepdims=True)
    f_x = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)
    return f_x

def predict():
    for i in range(0,200):
        image = cv2.imread('xxx/163.png')
        image = cv2.resize(image,(720,720)) # 与导出的onnx的input保持一致
        height,width = image.shape[:2]# H,W,C
        image = image/np.max(image)
        tmpImg = np.zeros((height,width,3))
        start = time.time()
        tmpImg[:,:,0] = (image[:,:,2]-0.406)/0.225
        tmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224
        tmpImg[:,:,2] = (image[:,:,0]-0.485)/0.229

        tmpImg = tmpImg.transpose((2, 0, 1)).astype(np.float32)# HWC->CHW
        tmpImg = tmpImg[np.newaxis,:,:,:]# CHW->NCHW

        # 推理
        # providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
        providers=[ 'CPUExecutionProvider']

        onnx_session = onnxruntime.InferenceSession('/xxx/resnet34.onnx',providers=providers)
        onnx_input = {'data':tmpImg}
        onnx_output = onnx_session.run(['output'],onnx_input)[0]
        prob = softmax(onnx_output) # softmax 转换为置信度值
        print(time.time()-start) #   计算推理时间

# 从 0.67~0.61 优化到了 0.42~0.31
predict()
```

如果要用 GPU ONNX 进行推理的话，需要安装 GPU 版本，`pip install onnxruntime-gpu` 

## TensorRT

直接用库。https://github.com/NVIDIA-AI-IOT/torch2trt

```python
import torchvision
import torch
import time
from torch2trt import torch2trt

model = torchvision.models.resnet34(pretrained=False).cuda().half().eval()
data = torch.randn((1, 3, 224, 224)).cuda().half()
print("start trt")
tmp = time.time()
model_trt = torch2trt(model, [data], fp16_mode=True)
print(f"finish trt:{time.time()-tmp}")

trt_start = time.time()
output_trt = model_trt(data) # tensorrt 0.001
print(time.time()-trt_start)

model_start = time.time()
output = model(data)
print(time.time()-model_start) # pytorch 0.005

torch.save(model_trt.state_dict(), 'resnet18_trt.pth')
print(output.flatten()[0:10])
print(output_trt.flatten()[0:10])
print('max error: %f' % float(torch.max(torch.abs(output - output_trt))))
```

# 加速 Numpy

`pip install numba`

@numba.njit 装饰在方法上，可以对方法内的 numpy 进行加速