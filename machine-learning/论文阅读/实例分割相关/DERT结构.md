## DETR结构

整个DETR的结构十分简单，如图二所示。它包含三个主要组件。

- CNN backbone提取compact特征表示
- encoder-decoder transformer
- 一个进行最终检测的简单前馈网络（FFN）

![image-20210417160102874](..\pics\CV\transformer\DETR.png)

> **Backbone**

初始图像大小$x_{img}∈ \mathbb{R}^{C*H*W}$（三通道的彩色图片），CNN主干网络为其删除低分辨率的activation map$f∈\mathbb{R}^{C*H*W}$。我们使用的典型值是$C=2048$ and $H,W = \frac{H_0}{32},\frac{W_0}{32}$

> **transformer encoder**

首先，$1*1$的卷积将high-level activation map f从C个通道减少到更小的维数d，从而生成一个新的特征图$z_0∈\mathbb{R}^{d*H*W}$,编码器需要一个序列作为输入，因此我们将$z_0$的空间尺寸折叠为一维的，从而生成$d*HW$的特征图。每个编码层都有一个标准的体系结构，由一个multi-head self-attention模块和一个前馈网络（FFN）组成。因为transormer是`特征之间没有空间位置关系`，因此我们提供了一个固定位置编码来进行补充，该编码被添加到每个attention层中。我们遵循补充材料中的体系结构的详细定义，它遵循了中所描述的定义。

> **transformer decoder**

解码器遵循标准的transformer结构，使用multi-headed self- and encoder-decoder attention机制。`与原始的转换器的不同之处在于，我们的模型在每个解码器并行解码N个对象`，而Vaswani等人则是，使用自回归模型，该模型一次预测一个元素的输出序列（NLP由于前后的相关性好像只能一次预测一个？）我们把这些补充材料推荐给不熟悉这些概念的读者。由于解码器也是`特征之间没有空间位置关系`，因此N个输入嵌入必须不同才能产生不同的结果。这些输入嵌入是学习的位置编码，我们将其称为对象查询，并且类似于编码器，我们将它们添加到每个关注层的输入中。 N个对象查询由解码器转换为嵌入的输出。 然后，它们通过前馈网络（在下一个小节中描述）独立地解码为框坐标和类标签，从而得出N个最终预测。 通过对这些嵌入的自编码器和解码器注意，模型可以使用它们之间的成对关系全局地将所有对象归为一类，同时能够将整个图像用作上下文。 

> **FFNs** Prediction feed-forward networks 

最终的预测是通过一个带有ReLU激活函数和隐藏维数d的3层感知器和一个线性投影层来计算的。FFN预测归一化的中心坐标、输入图像框的高度和宽度，线性层使用softmax函数预测类标签。由于我们预测一个固定大小的N个包围框，其中N通常比图像中实际感兴趣对象的数量大得多，因此使用额外的特殊类标签∅表示槽内没有检测到对象。这个类在标准的对象检测方法中扮演类似于“background”类的角色。



