> transformer

![](https://pic2.zhimg.com/v2-1be61511d53dca07f1c83697eb23a87d_r.jpg)

**Transformer encoder**部分首先将输入的特征图降维并flatten，然后送入下图左半部分所示的结构中，和空间位置编码一起并行经过多个自注意力分支、正则化和FFN，得到一组长度为N的预测目标序列。

接着，将Transformer encoder得到的预测目标序列经过上图右半部分所示的Transformer decoder，并行的解码得到输出序列（而不是像机器翻译那样逐个元素输出）。每个层可以解码N个目标，由于解码器的位置不变性，即调换输入顺序结果不变，除了每个像素本身的信息，位置信息也很重要，所以这N个输入嵌入必须不同以产生不同的结果，所以学习NLP里面的方法，加入positional encoding并且每层都加**作者非常用力的在处理position的问题，在使用 transformer 处理图片类的输入的时候，一定要注意position的问题。**

>预测头部（FFN）

使用共享参数的FFNs（由一个具有ReLU激活函数和d维隐藏层的3层感知器和一个线性投影层构成）独立解码为包含类别得分和预测框坐标的最终检测结果（N个），FFN预测框的标准化中心坐标，高度和宽度w.r.t. 输入图像，然后线性层使用softmax函数预测类标签。



# DETR模型

直接集预测中有两个要素是必不可少的：

（1）一组预测损失，迫使预测的和真实的boxes进行唯一匹配。

（2）一种架构来预测一组对象并为它们之间的关系建模。我们在图二中详细描述我们的架构。

![image-20210417160102874](..\..\pics\CV\transformer\DETR.png)

## Object detection set prediction loss

对象检测集损失。

DETR在一次通过解码器的过程中，会推断出固定大小的N个预测集，其中N被设置为显着大于图像中对象的典型数量。 训练的主要困难之一是根据地面真相对预测的对象（类，位置，大小）进行评分。 我们的损失会在预测的和ground true之间产生最佳的二分匹配，然后优化特定于对象的（边界框）损失。 

我们用y表示真实对象的集合，$\hat y = \{\hat y_i\}^N_{i=1}$表示N个预测的集合。假设N大于图像中的对象数，我们页将y视为大小为N的集合，并用空对象进行填充。为了找到这两个集合之间的二分匹配，我们搜索具有最低成本的N个元素σ属于$S_n$的排列

> 找集合的二分匹配，使这个匹配最合适

![image-20210417160648998](..\..\pics\CV\transformer\(1))

其中$L_{match}(y_i,\hat y_{σ(i)})$是ground true $y_i$ 和具有索引$σ(i)$的预测之间的成对匹配成本。遵循先前的工作，使用匈牙利算法可以有效地计算出最佳分配 

![image-20210417161413428](..\..\pics\CV\transformer\(1)_word)

匹配成本同时考虑了类别预测及groudn true和预测值的相似性。每个groudn truth集合的元素i可以看作是$y_i=(c_i,b_i)$,$c_i$是类别标签（可能是空对象），$b_i∈[0,1]^4$是一个向量，`这个向量被定义为groudn truth box的盒中心坐标及相对于图像大小的高度和宽度。`对于预测的$σ(i)$我们可以看作是类别$c_i$ as $\hat p_{σ(i)}(c_i)$和预测框$\hat b_{σ(i)}$. 我们将$L_{match}(y_i,\hat y_{σ(i)})$定义为如下式子

> 二分匹配的计算方式

![image-20210417162426808](..\..\pics\CV\transformer\L_match)

这种寻找匹配的过程与现代探测器中用于匹配区域或锚定的启发式分配规则具有相同的作用。`主要的区别是，我们需要为没有重复的直接集预测找到一对一的匹配。`

第二步是计算损失函数，即上一步中匹配的所有配对的匈牙利损失。 我们将损失定义为类似于常见对象检测器的损失，即用于类预测的`负对数似然`和稍后定义的`框损失`的`线性组合`

> 用二分匹配的最佳结果进行损失计算。

![image-20210417162952354](..\..\pics\CV\transformer\(2).png)

$\hat σ$是上一步计算出的最佳匹配。实际上，当$c_i= \empty$时，`我们将log-probability项的权重降低了10倍，以解决类别不平衡问题。`这类似于Faster R-CNN的训练过程如何通过分段抽样平衡positive/negative proposals。`注意，对象与∅的匹配代价不依赖于预测，在这种情况下代价为常数。`在匹配成本中，我们使用概率$\hat p_{\hatσ(i)}(ci)$代替对数概率(log-probabilities)。 这使得类预测项可与$L_box(.,.)$相称（如下所述），并且我们观察到了更好的经验性能 

**Bounding box loss：**匹配成本和匈牙利损失的第二部分是$L_{box}(.)$,bounding box的scores. `不像多数检测的box预测那样，先做一些初始化的猜测，我们是直接做的box预测。`尽管这种方法简化了实现，但是却带来了损失相对缩放的问题（relative scaling of the loss）。即使大小相对误差相似，最常见的$l1$损失对大小盒子的比例也会有所不同。 为了减轻这个问题，我们使用了$l1$损失和广义的IOU损失$L_{iou}(.,.)$的线性组合，该比例不变（应该是系数都是1的意思？看看代码怎么写的！）总的来说，我们的box损失是$L_{box}(b_i,\hat b_{σ(i)})$定义为：

![image-20210417170336776](..\..\pics\CV\transformer\L_box.png)

$入_{iou}, 入_{L1}∈R$是超参数。这两个损失通过批处理中对象的数量进行归一化。



## DETR结构

整个DETR的结构十分简单，如图二所示。它包含三个主要组件。

- CNN backbone提取compact特征表示
- encoder-decoder transformer
- 一个进行最终检测的简单前馈网络（FFN）

![image-20210417160102874](..\..\pics\CV\transformer\DETR.png)

与许多的现代检测不同，DETR可以在任何深度学习框架中实施，该宽街可提供通用的CNN主干和仅几百行的变压体系结构。DETR的推理代码可以在PyTorch中用少于50行实现！我们希望我们的方法的简便性将吸引新的研究人员进入检测领域。

> **Backbone**

初始图像大小$x_{img}∈ \mathbb{R}^{C*H*W}$（三通道的彩色图片），CNN主干网络为其删除低分辨率的activation map$f∈\mathbb{R}^{C*H*W}$。我们使用的典型值是$C=2048$ and $H,W = \frac{H_0}{32},\frac{W_0}{32}$

> **transformer encoder**

首先，$1*1$的卷积将high-level activation map f从C个通道减少到更小的维数d，从而生成一个新的特征图$z_0∈\mathbb{R}^{d*H*W}$,编码器需要一个序列作为输入，因此我们将$z_0$的空间尺寸折叠为一维的，从而生成$d*HW$的特征图。每个编码层都有一个标准的体系结构，由一个multi-head self-attention模块和一个前馈网络（FFN）组成。因为transormer是`特征之间没有空间位置关系`，因此我们提供了一个固定位置编码来进行补充，该编码被添加到每个attention层中。我们遵循补充材料中的体系结构的详细定义，它遵循了中所描述的定义。

> **transformer decoder**

解码器遵循标准的transformer结构，使用multi-headed self- and encoder-decoder attention机制。`与原始的转换器的不同之处在于，我们的模型在每个解码器并行解码N个对象`，而Vaswani等人则是，使用自回归模型，该模型一次预测一个元素的输出序列（NLP由于前后的相关性好像只能一次预测一个？）我们把这些补充材料推荐给不熟悉这些概念的读者。由于解码器也是`特征之间没有空间位置关系`，因此N个输入嵌入必须不同才能产生不同的结果。这些输入嵌入是学习的位置编码，我们将其称为对象查询，并且类似于编码器，我们将它们添加到每个关注层的输入中。 N个对象查询由解码器转换为嵌入的输出。 然后，它们通过前馈网络（在下一个小节中描述）独立地解码为框坐标和类标签，从而得出N个最终预测。 通过对这些嵌入的自编码器和解码器注意，模型可以使用它们之间的成对关系全局地将所有对象归为一类，同时能够将整个图像用作上下文。 

> **FFNs**Prediction feed-forward networks 

最终的预测是通过一个带有ReLU激活函数和隐藏维数d的3层感知器和一个线性投影层来计算的。FFN预测归一化的中心坐标、输入图像框的高度和宽度，线性层使用softmax函数预测类标签。由于我们预测一个固定大小的N个包围框，其中N通常比图像中实际感兴趣对象的数量大得多，因此使用额外的特殊类标签∅表示槽内没有检测到对象。这个类在标准的对象检测方法中扮演类似于“background”类的角色。

> **Auxiliary decoding losses**

我们发现在训练期间在解码器中使用辅助损耗[1]很有帮助，特别是有助于模型输出正确数量的每个类的对象。 我们在每个解码器层之后添加预测FFN和匈牙利损失。 所有预测FFN均共享其参数。 我们使用附加的共享层范数来标准化来自不同解码器层的预测FFN的输入。 

