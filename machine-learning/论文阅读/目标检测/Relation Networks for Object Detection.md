# Relation Networks for Object Detection

CVPR 2018

# 疑问

{r1，r2}={1，1}啥意思

# Abstract

尽管人们多年来一直认为，对物体之间的关系进行建模将有助于物体识别，但没有证据表明这一想法在深度学习时代行得通。所有最先进的对象检测系统仍然依赖于单独识别对象实例，而不是在学习过程中利用它们的关系。

本文提出了一个对象关系模型。<span style="color:green">它通过一组对象的外观特征（appearance feature）和几何（geometry）之间的交互来同时处理这些对象</span>，从而允许对它们的关系进行建模。它重量轻，位置合适。它不需要额外的监管，易于嵌入现有网络。在现代目标检测流水线中，它在改进目标识别和重复消除步骤方面是有效的。验证了在基于CNN的检测中建模对象关系的有效性。它产生了第一个完全端到端的物体检测器。代码可从以下网址获得：

- <a href="https://github.com/msracver/Deformable-ConvNets">官方代码地址</a>
- <a href="https://github.com/heefe92/Relation_Networks-pytorch">pytorch实现</a>

# Introduction

近年来，使用深度卷积神经网络(CNNs)的目标检测取得了重大进展。最先进的目标检测方法大多遵循基于区域的。给定一组稀疏的region proposals，分别对每个建议执行对象分类和包围盒回归。然后应用启发式和手工制作的后处理步骤，非最大抑制(NMS)，以消除重复检测。

多年来，视觉社区已经很好地认识到，上下文信息或对象之间的关系有助于对象识别。这样的作品大多是在深度学习盛行之前。在深度学习时代，利用对象关系进行检测学习没有显著的进展。大多数方法仍然侧重于分别识别对象。

一个原因是对象-对象关系很难建模。这些对象位于不同类别、不同比例的任意图像位置，并且它们的数量在不同图像之间可能有所不同。现代基于CNN的方法大多具有简单规则的网络结构。目前还不清楚如何适应现有方法中的上述不规则性。

我们的方法是由自然语言处理领域中注意力模块的成功所推动的。注意模块可以通过聚集来自一组元素(例如，源句子中的所有单词)的信息(或特征)来影响单个元素(例如，机器翻译中的目标句子中的单词)。由任务目标驱动，自动学习聚合权重（aggregation weights）。<span style="color:green">注意力模块可以对元素之间的依赖性进行建模，而无需对它们的位置和特征分布做过多的假设。</span>最近，注意力模块已经成功地应用于视觉问题，例如图像字幕（ image captioning）。

在这项工作中，我们首次提出了一个适用于目标检测的注意模块。它建立在一个基本的注意力模块上。一个明显的区别是，原始元素是对象，而不是单词。这些物体具有2D空间排列和比例/纵横比的变化。它们的位置，或者一般意义上的几何特征，比1D句子中的位置这个词起着更加复杂和重要的作用。<span style="color:green">因此，所提出的模块将原来的注意力权重扩展为两个分量: 原来的权重和新的几何权重。后者对对象之间的空间关系建模，并且只考虑它们之间的相对几何形状，使得模块平移不变，这是对象识别的理想特性。新的几何权重在我们的实验中证明很重要。</span>

该模块称为对象关系模块。它与注意力模块有着相同的优势。它采用可变数量的输入，并行运行（速度快），<span style="color:green">完全可微，并且是原地运行(输入和输出之间没有维度变化)</span>。因此，它作为一个基本的构建模块，可以灵活地用于任何体系结构。<span style="color:red">【可作为一个基本构建模块，灵活运用于任何体系】</span>

具体来说，它被应用于几个最先进的目标检测框架，并显示出一致的改进。如图1所示，它用于改进实例识别步骤和学习重复删除步骤(详见第4.1节)。对于对象检测，关系模块支持所有对象的联合推理，并提高识别精度(第4.2节)。为了消除重复，<span style="color:green">传统的NMS方法被一个轻量级的关系网络(第4.3节)所取代和改进，产生了第一个端到端的对象检测器(第4.4节)。</span>

<img src="../../pics\CV\ISG\Relation Networks for Object Detection\image-20210524175326416.png">

原则上，我们的方法与大多数基于CNN的目标检测方法有着根本的不同，是对它们的补充。<span style="color:green">它利用了一个新的维度: 一组对象被同时处理、推理和相互影响，而不是单独识别。</span>

对象关系模块是通用的，不限于对象检测。可以用于：如实例分割、动作识别、对象关系检测、标题、VQA等。

# Related Works

## Object relation in post-processing

大多数先前的工作使用对象关系作为后处理步骤。通过考虑对象关系，对检测到的对象重新评分。<span style="color:green">例如，DPM使用`共现`来细化对象分数，共现表示两个对象类在同一图像中存在的可能性。</span>随后的方法通过考虑额外的位置和大小，尝试更复杂的关系模型。关于更详细的调查，我们请读者参考<a href="">文献18</a>。这些方法在前深度学习时代取得了一定的成功，但在深度学习时代并不有效。<span style="color:green">一个可能的原因是深层转换网络通过大的感受野隐含地整合了上下文信息。</span>

## Sequential relation modeling

顺序关系建模

最近的几个工作使用LSTM和SMN（空间记忆网络）来建模对象关系。在检测过程中，较早检测到的对象用于帮助查找下一个对象（顺序执行的）。更重要的是，它们没有显示出改进SOTA的对象检测方法的证据（这些方法是简单的前馈网络simple FFN）。

<span style="color:green">相比之下，我们的方法对于多个对象是并行的。它自然适合并改进了现代物体探测器。</span>

## Human centered scenarios

相当多的作品关注human-object的关系。它们通常需要额外的关系注释，比如人类行为。相比之下，我们的方法对于object-object关系是通用的，不需要额外的监督。

## Duplicate removal

尽管使用深度学习的目标检测取得了显著的进步，但完成这一任务的最有效的方法仍然是greedy和hand-crafted的非极大抑制(NMS)及其soft version。<span style="color:green">这个任务自然需要关系建模。例如，NMS使用边界框和分数之间的简单关系。</span>

最近，GossipNet试图通过将一组对象作为一个整体进行处理来学习重复删除，因此与我们有着相似的精神。然而，它的网络是专门为任务设计的，非常复杂(深度>80)。其精度与NMS相当，但计算成本很高。虽然原则上允许端到端学习，但没有实验证据。

相比之下，我们的关系模块简单、通用，并且作为一个应用程序应用于重复删除。我们的去重网络简单得多，计算开销小，超过了SoftNMS。更重要的是，我们首次证明了端到端的对象检测学习是可行和有效的。

## Attention modules in NLP and physical system modeling
注意力模块最近已成功应用于自然语言处理领域和物理系统建模。注意力模块可以很好地捕捉这些问题中的长期依赖性。在自然语言处理中，最近的趋势是用注意力模型代替递归神经网络，实现并行化实现和更有效的学习。

我们的方法就是受这些作品的启发。我们将注意力建模扩展到目标检测这一重要问题。对于建模视觉对象关系，它们的位置，或者一般意义上的几何特征，起着复杂而重要的作用。<span style="color:green">因此，所提出的模块引入了新的几何权重来捕捉对象之间的空间关系。新的几何权重是平移不变的，这是可视化建模的一个重要特性。</span>

# Object Relation Module

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210524201753975.png">

我们首先回顾一个基本的注意力模块，叫做<span style="color:green">“缩放点积注意力”（缩放点积可以解决尺度归一化的问题）</span>。输入由维度$d_k$的查询和键以及维度$d_v$的值组成。在查询和所有关键字之间执行点积，以获得它们的相似性。应用`softmax`函数来获得值的权重。给定一个查询q，所有keys (打包成矩阵K) 和 value(打包成V)，输出值是输入值的加权平均值：
$$
v^{out} = softmax(\frac{qK^t}{\sqrt{d_k}})V-----(1)
$$
我们现在描述对象关系计算。让一个物体由它的几何特征$f_G$和外观特征$f_A$组成。在这项工作中，$f_G$只是一个简单的4维对象边界框，并决定任务（Section 4.2 and 4.3）

<span style="color:green">给定N个对象的输入集合${ (f^n_A, f^n_G) }^N_{n=1}$,整个对象集合相对于第n个对象的关系特征$f_R(n)$计算如下：</span>
$$
f_R(n) = \sum_{m}w^{mn}*(W_V*f^m_A)   -----(2)
$$
输出是来自其他对象的外观特征的加权和，由$W_V$线性变换(对应于等式中的值V (1))。关系权重$w_{mn}$表示来自其他物体的影响。其中$w_{mn}$的计算如下：
$$
w^{mn} = \frac{w_{G}^{mn}*exp(w_{A}^{mn})}{\sum_{k}w^{kn}_{G}*exp(w^{kn}_A)}----- (3)
$$
Appearance weight $w^{mn}_A$的计算类似于公式一的缩放点积。
$$
w^{mn}_{A} = \frac{dot(W_Kf^m_A, W_Qf^n_A)}{\sqrt{d_k}}
$$
$W_k$和$W_Q$矩阵扮演着等式1中的K和Q的角色。

他们将原始特征$f^m_A$和$f^n_A$投影到子空间中，以测量它们的匹配程度。投影后的特征尺寸为$d_k$。

几何权重计算如下：
$$
w^{mn}_G = max \{ 0, W_G* \varepsilon_G(f^m_G,f^n_G) \} -----(5)
$$

----

有两个步骤。

- 首先，将两个对象的几何特征嵌入到$ \varepsilon_G$这个高维表示中。为了使其对平移和比例变换不变，使用了四维相对几何特征。四维相对几何特征：$(log(\frac{|x_m-x_n|}{w_m}),log(\frac{|y_m-y_n|}{h_m}),log(\frac{w_n}{w_m}),log(\frac{h_n}{h_m}))$，这种四维特征通过[56]中的方法嵌入到高维表示中，该方法计算不同波长的余弦和正弦函数。嵌入后的特征维数为$d_g$。
- 第二，嵌入特征由$W_G$转为一标量权重并被修剪为0，acting as a ReLU nonlinearity。零修剪操作仅限制某些几何关系的对象之间的关系。

---

因为使用了几何权重（等式5）和注意力权重（等式3）使得我们的方法与基本的注意力（等式1）不同。为了验证等式5的有效性，我们设计了两个更简单的变体。

- 第一种变体 called none，没有使用等式5的几何权重。$w^{mn}_G$在等式3中是一个常量1.
- 第二种叫 unary。它采用的是最近一些工作的方法。具体来说就是，$f_G$以相同的方式嵌入到了一个高纬空间中（像$f_A$），并添加到$f_A$上形成新的appearance feature。没有计算注意力权重的意思嘛（The attention weight is then computed as none method）。表1（a）和 section 5.2证明了我们设置的几何权重的有效性。

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526104220881.png">

一个对象关系模块聚集中的$N_r$关系特征，并通过相加来增强输入对象的appearance feature。相加的公式如下：
$$
f^n_A = f^n_A + Concat[ f^1_R(n),...,f^{N_r}_R(n)], for \ all \ n ----- (6)
$$
$Concat(.)$用于聚合多个关系特征。为了匹配通道维度，每个$W^r_V$输出通道设置为输入feature $f^m_A$ 的 $\frac{1}{N_r}$

对象关系模块的等式6总结了算法1。通过基础操作它很容易就是实现了，如图二所示。

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526105354225.png">

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210524201753975.png">

等式2中的每个关系函数是由四个矩阵$(W_k,W_Q,W_G,W_V)$参数化的来的，一共$4N_r$个。设$d_f$是$f_A$的输入特征。参数的数量是：
$$
O(Space) = N_r(2d_fd_k + d_g)+d^2_f ----(7)
$$
按照算法1，计算复杂度是：
$$
O(Comp.) = Nd_f(2N_rd_k+d_f)+N^2N_r(d_g+d_k+d_f/N_r+1)----(8)
$$
典型参数值为$N_r= 16，d_k= 64，d_g= 64$。一般来说，$N和d_f$通常在数百的规模。当应用于目标检测时，总的计算开销很低。

关系模块具有相同的输入和输出维度，因此可以被视为在任何网络架构中就地使用的基本构建块。它是完全可微的，因此可以很容易地通过反向传播进行优化。下面是它在目标检测系统中的应用。

# Relation Networks For Object Detection

## Review of Object Detection Pipeline

这项工作符合基于区域的对象检测范式。该范式建立在开创性的工作R-CNN上和大多数的目标检测框架上。如这里所总结的，在所有以前的工作中都使用了四个步骤。

- 第一步：生成完整的图片feature。通过CNN的backbone从图像中提取完整的特征图。（通常是原图大小的$\frac{1}{16}$，或者更小）。backbone在ImageNet分类任务上进行预训练，在检测训练中进行微调。
- 第二步：生成regional features。从卷积特征和一组稀疏的region proposals中，RoI pooling层提取固定分辨率的区域特征。
- 第三步：执行实例识别。根据每个proposal regional特征，一个 head network预测候选区域的类别概率并通过回归细化bounding box。这个网络通常比较浅，随机初始化参数，在检测训练时和backbone一起训练。
- 第四步：执行重复删除。每个对象应只检测一次，重复的检测应该被删除。这个过程一般本称为非极大值抑制（NMS）。虽然NMS在实践中运行良好，但是它是手动设计的，不是最佳的。它阻碍了对对象进行端到端学习。

在这项工作中，所提出的对象关系模块用于最后两个步骤：<span style="color:green">实例识别 and 重复删除</span>。我们的方法确实增加了最后两步的效果。且这两步很容易单独或联合训练。联合训练进一步提高了精度，并产生了第一个端到端通用目标检测系统。

为了验证方法的有效性，用现有的最佳网络结构（ResNet）和最好的检测器（Faster R-CNN，FPN，deformable convolutional network（DCN））。RPN用于生成region proposal。

- Faster R-CNN：ResNet作为backbone。conv4上用RPN提取region proposal。随后实例识别头网络在conv5后用256-d的$1*1$卷积进行降维。注意，按照惯例，conv5中的步幅从2变为1
- FPN：与Faster RCNN相比，它构建了一个特征金字塔，促进不同规模的端到端的学习。RPN和head网络应用于金字塔中所有比例的特征。我们遵循中的培训细节【参考文献37】
- DCN：与 Faster RCNN相比，<span style="color:green">它通过用可变形卷积层替换conv5中的最后几个卷积层来修改主干网络（可变形卷积有啥作用）</span>。它还用可变形RoI池取代了标准RoI池。我们遵循中的培训细节【参考文献11】

尽管存在差异，但上述架构的一个共同点是它们都采用相同的头网络结构，即Roi汇集的区域特征经历两个完全连接的层，以生成用于proposal分类和边界框回归的最终特征。

下面，我们展示了关系模块可以使用两个全连接head来增强实例识别步骤。

## Relation for Instance Recognition

给定第n个proposal的RoI pooled特征，对它使用两个维度为1024的全连接层。通过线性层实现分类和bounding box回归。过程总结如下：

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526114311179.png">

对象关系模块（算法1）可以在不改变特征尺寸的情况下转换所有proposals的1024-d特征。所以可以在等式9的任意fc层后使用任意次数。这种增强的2fc+RM head如图三所示。总结图如（10）所示

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526114743988.png">

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526114702183.png">

在等式（10）中，$r_1$和$r_2$表示进行多少次relation module。请注意，<span style="color:green">关系模块还需要所有proposals的边界框作为输入</span>。为了清楚起见，这里忽略了这个符号。

添加关系模块确实提高了识别的准确度。在消融实验中得到了验证。

## Relation for Duplicate Removal

消除重复的任务需要利用对象之间的关系。NMS是一个简单的例子，得分高的对象将相邻的得分低的擦出（几何关系）。

尽管简单，greedy nature和在NMS中手工选择参数使它成为一个明显的次优选择。下面，我们展示了proposed relation模块可以学习一种更简单但更有效的方式删除重复。

去重是一个二分类问题。对于每个ground truth，只有一个与其（最）匹配的被分类为正确。其他匹配项被认为是重复的。

这种分类是通过网络执行的，如图3（b）所示。输入是一组检测到的对象。每个对象都有最终的1024-d特征、分类分数和bounding box。网络会为每个对象输出一个二分类的概率$s1∈[1,0]$(1是正确，0是重复)。两个分数$s_0$和$s_1$的乘积就是最后的分类分数。因此，一个好的检测器这两个分数应该都很大。

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526151921268.png">

这个网络有三步。

- 第一，融合1024-d的特征和分类得分，生成appearance feature。
- 第二，关系模块转换所有对象的这些appearance features。
- 第三，每个对象的transformed features通过一个线性分类器（$W_s$ in Figure 3(b) ）和sigmoid输出概率$∈[0,1]$。

关系模块是网络的核心。它确保了使用来自于多个来源（边界框、原始外观特征和分类分数）的信息有效的的端到端的学习。此外，分类分数的使用也很重要。

**Rank feature：**我们发现最有效的方法是将分数转化为等级，而不是直接使用他们的值。具体来说，输入的N个对象按照得分降序排列。每个对象都相应地被赋予一个等级$∈[1,N]$。然后，使用与第3节中几何特征嵌入相同的方法，将标量等级嵌入到更高维度的128维特征中。

等级特征和原始1024-d appearance feature都被转换为128-d (分别通过图3(b)中的$W_{f_R} $and $W_{f}$)，并作为输入添加到关系模块。

**Which object is correct：**给定一些检测到的物体，无法立即判断清楚哪一个应该与ground truth对象匹配才是正确的。最明显的选择是遵循Pascal VOC 或 COCO数据集的评估标准。（废话啊）也就是说，给定detection boxes和ground truth box之间的IoU的预定义阈值$η$，$IoU>=η$首先被匹配到相同的ground truth。最高分的detection box是正确的，其他的是重复的。

因此，当学习和评估使用相同的阈值η时，这种选择标准最有效。例如，在学习中使用η  = 0.5产生最佳mAP@0.5度量，但不产生mAP @0.75。这在表4中得到验证。

这一观察表明，我们的方法有一个独特的优点，这是NMS所缺少的:可以根据需要自适应地学习重复删除步骤，而不是使用预设的参数。例如，当需要高定位精度时，应使用较大的η。

<span style="color:red">受COCO验证标准的影响，我们最佳训练使用的是多个阈值$η∈ \{  0.5，0.6，0.7，0.8，0.9 \}$。具体来说，图3(b)中的分类器$W_s$被改变以输出对应于不同IoU阈值和正确检测的多个概率，生成多个二进制损失项。</span>不同案例之间的训练非常均衡。在推理过程中，多个概率被简单地平均为单个输出。

**Training：**二进制交叉熵损失被用于计算最后的得分（两个分数的乘积，将图3(b) ）。损失是对所有对象类别的所有detection boxes的损失进行平均。（意思是，5个图，100个对象，计算这100个对象的平均损失嘛？）<span style="color:red">为所有对象类别训练单个网络。（这句话没明白）</span>

重复分类问题是极其不平衡的（一个对象被多次使用？）。多数的检测是重复的。正确detection的比例通常低于0.01。然而，我们发现简单的交叉熵损失很有效。这归因于最终得分$s_0*s_1$中的乘法行为。因为大多数的detections $s_0$很小，这会导致$s_0*s_1$也很小。他们损失的价值大小$L = -log(1-s_0*s_1)$(对于不正确的物体)，反向传播的梯度结果$\partial L/\partial s_1 = s_0/(1-s_0*s_1)$的幅度也会非常小，不会对优化产生太大的影响。<span style="color:green">直观地说，训练集中在几个具有大$s_0$的real duplicate detecions上。这与最近的焦点损失工作有着相似的精神，在焦点损失工作中，大多数无关紧要的损失项被向下加权，并在优化过程中发挥次要作用。</span>

**Inference：**相同的重复删除网络独立应用于所有对象类别。乍一看，当对象类(COCO数据集[39]为80)和检测(N = 300)的数量很高时，运行时的复杂性可能很高。然而，在实践中，在大多数对象类别中，大多数检测的原始分数s0接近0。例如，在表4中的实验中，只有12.0%的类的检测得分> 0.01，而在这些类中，只有6.8%的检测得分> 0.01。

<span style="color:green">去除这些无关紧要的类别和检测后，最终的识别精度不受影响。</span>在剩余的检测上运行重复删除网络是可行的，在Titan X GPU上需要大约2毫秒。请注意，NMS和软NMS方法是顺序的，在一个中央处理器上大约需要5毫秒。还要注意的是，最近的学习NMS的工作[30]使用了一个非常深刻和复杂的网络(深度高达80)，比我们的效率低得多。

## End-to-End Object Dection

重复删除网络在第4.3节中单独训练。然而，没有什么可以阻止端到端的训练。如图3 (b)中的红色箭头所示，反向传播的梯度可以传入原始的1024-d特征和分类分数，这可以进一步传播回头部和主干网络。

<span style="color:red">我们的端到端训练简单地结合了区域建议损失</span>、第4.2节中的实例识别损失和第4.3节中的重复分类损失，权重相同。比如识别，要么原始head 等式(9)或增强head 等式(10)可以使用。

端到端的培训显然是可行的，但它有效吗？乍一看，有两个问题。

首先，实例识别步骤和重复删除步骤的目标似乎是矛盾的。前者期望所有匹配到同一个地面真实对象的对象都有高分。后者预计只有一个人会这么做。在我们的实验中，我们发现端到端训练效果很好，并且与第4.2节和第4.3节中单独训练时相比，两个网络的收敛速度相同。我们相信，这种看似矛盾的冲突再次通过最终得分$s_0*s_1$的倍增行为得到了调和，<span style="color:green">这使得两个目标相辅相成，而不是相互冲突。实例识别步骤只需要为好的检测产生高分$s_0$(不管是否重复)。重复项删除步骤只需要为重复项生成较低的得分$s_1$。只要两个分数中的一个正确，大多数非对象或重复检测都是正确的。</span>

第二，在去重步骤中的二进制分类的ground truth标签依赖于实例识别的输出，并且在端到端训练过程中改变。然而，在实验中，我们没有观察到这种不稳定性造成的不利影响。虽然还没有理论上的证据，但我们的猜测是，重复删除网络相对容易训练，不稳定的标签可能是一种正则化的手段。

实验证明(第5.3节)，端到端训练提高了识别精度。

# Experiments

所有实验都是在COCO目标检测数据集（80个类别）上做的。80k训练图像和35k  val图像子集的联合用于训练。大多数消融实验报告了5k个未使用的val图像(表示为minival)子集的检测精度，这是通常的做法。

对于主干网，我们使用ResNet-50和ResNet101。除非另有说明，否则使用ResNet-50。

## Relation for Instance Recognition

在本节中，IoU阈值为0.6的NMS用于所有实验的重复去除。

**Relation module improves instance recognition：**

表1比较了等式9中3fc这个基线，使用推荐的等式10 2fc+RM、各种参数下的结果如下：

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526164425210.png">

我们首先注意到，与文献相比，我们的基线实施达到了合理的准确性(29.6 mAP)(例如，[37]使用ResNet-50的报告28.0和[11]使用ResNet-101的报告29.4)。

消融研究基于三个关键参数。

**usage of geometric feature**几何特征的使用。如第3节所分析的，我们在等式5中使用几何特征与两个简单的实现进行了比较。结果表明，我们的方法是最好的，尽管这三个都超过了基线。

**number of relations $N_{r}$**使用更多的关系可以提高准确性。$N_r = 16$时效果最佳。 +2.3mAP

**Number of modules**使用更多的关系模块稳步提高精度，最高可达+3.2 mAP增益。由于这也增加了参数和计算复杂性，默认情况下，r1= 1，r2= 1。

**改善是来自更多的参数还是深度？**

表2通过在宽度或深度上增强基线2fc head (a)来回答这个问题，使得它的复杂性与添加关系模块的复杂性相当。

- 更宽的2fc（1432-d）只带来了很小的提升（+0.1mAP）。 
- 3fc反而降低了（-0.6mAP），可能时由于训练的困难
- 为了使训练更容易，使用了残差块(d)，只提升了一点（+0.3mAP）
- 当使用全局上下文时(例如，在分类之前，2048-d全局平均汇集要素与第二个1024-d实例要素相连接)，没有提供改进。
- 相比之下，我们的方法(f)显著提高了精度(+2.3 mAP)。

我们还考虑了另一个基线，它将原始汇集的特征与来自2倍大RoI(g)【增大RoI特征图的大小】的特征连接起来。性能从29.6提高到了30.4mAP，表示利用上下文线索的更好方式。<span style="color:red">另外，我们把这个新的head和relation modules结合起来，就是用{r1，r2}={1，1} (h)代替2fc。我们得到32.5 mAP，比设置(f) (31.9 mAP)好0.6。这表明使用更大的窗口上下文和关系模块是互补的。</span>

当使用更多的剩余块并且头网络变得更深(I)时，精度不再增加。同时，当使用更多的关系模块时，准确性不断提高(j)

对比表明，该关系模型是有效的，其效果超出了增加网络容量的范围。

**Complexity**：在每个关系模块中$d_f = 1024, \ d_k = 64, \ d_g = 64$。当$N_r = 16$,根据等式7和8，一个模块大约有300万个参数和12亿次浮点运算。与表5中所示的整个检测网络的复杂性相比，计算开销相对较小（less than 2% for faster RCNN [44] / DCN [11] and about 8% for FPN [37]）。

## Relation for Duplicate Removal

本节中的所有实验都使用表1中的Faster RCNN基线 2fc head检测对象(top row，NMS后的29.6 mAP)来训练和推断我们在第4.3节中的方法。

在我们的方法中，关系模块参数设置为$d_f=128，d_k= 64，d_g=64，N_r= 16，N = 100。$使用较大的值不再增加准确性。去重网络有33万个参数，约3亿个FLOPs。这种开销很小，与使用ResNet-50的Faster RCNN基线网络相比，模型大小和计算量都约为1%。

表3调查了不同输入特征对关系模块的影响(图3 (b))。使用η = 0.5，我们的方法将mAP提高到30.3。不使用等级功能时，mAP降至26.6。当class score  $s_0$ 以类似的方式取代排名时(分数嵌入到128-d)，mAP下降到28.3。当不使用1024-d外观功能时，mAP略降至29.9。<span style="color:green">这些结果表明，等级特征对最终的准确性至关重要。</span>

当不使用几何特征时，mAP降至28.1。当使用第3节和表1  (a)中提到的unary法时，mAP降至28.2。这些结果验证了我们使用几何权重等式5的有效性。

**Comparison to NMS：**表4将我们的方法与NMS方法及其更好的SoftNMS进行了比较，后者也是最先进的重复消除方法。

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526172507899.png">

## End-to-End Object Detection



# Conclusions

无法解释为什么有效。但是直观感受这个方法是有效的。看这个结果图，会发现，这个人对手套的检测是有所贡献的。

<img src="..\..\pics\CV\ISG\Relation Networks for Object Detection\image-20210526164022882.png">

# Training Details

前2/3次迭代的学习率设置为：$2*10^{-3}$，后1/3次迭代的学习率设置为：$2*10^{-4}$（3个epoch）

FPN：图像短边设置为800像素，number region proposals=1000，anchors采用5个尺度和3个长宽比。区域建议网络经过大约170k次迭代(3个epoch)的训练。

实例识别(第5.1节)和端到端训练(第5.3节)都有340千次迭代(6个epoch)。前2/3次迭代的学习率设置为：$5*10^{-3}$，后1/3次迭代的学习率设置为：$5*10^{-4}$（3个epoch）

SGD：weight decay=$1*10^{-4}$,momentum=0.9，采用了与类无关的包围盒回归[10]，因为它具有与类感知版本相当的精度，但效率更高。（Class agnostic bounding box regression [10] is adopted as it has comparable accuracy with the class aware version but higher efficiency.）

对实例识别子网络：