[循环神经网络入门](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

# 卷积神经网络

## 资料

> 各种卷积的动图

https://github.com/vdumoulin/conv_arithmetic/tree/master/gif

## 卷积与全连接

> 全连接

神经网络相邻层的节点是全连接的，也就是输出层的每个节点会与输入层的<span style="color:red">所有节点连接</span>。

> 卷积

- 局部连接
- 权值共享

![image-20210405103651039](..\pics\CV\ISG\视觉方向\image-20210405103651039.png)

具体来说，卷积层是通过特定数目的卷积核（又称滤波器）对输入的多通道（channel）特征图进行扫描和运算，从而得到多个拥有更高层语义信息的输出特征图（通道数目等于卷积核个数）。图1.2形象地描绘了卷积操作的基本过程：下方的绿色方格为输入特征图，带灰色阴影部分是卷积核施加的区域；卷积核不断地扫描整个输入特征图，最终得到输出特征图，也就是上方的棕色方格。需要说明的是，输入特征图四周的虚线透明方格，是卷积核在扫描过程中，为了保证输出特征图的尺寸满足特定要求，而对输入特征图进行的边界填充（padding），一般可以用全零行/列来实现。

![image-20210405103811552](..\pics\CV\ISG\视觉方向\image-20210405103811552.png)

- 局部连接：卷积核尺寸远小于输入特征图的尺寸，输出层上的每个节点都只与输入层的部分节点连接
- 权值共享：卷积核的滑动窗机制，使得输出层上不同位置的节点与输入层的连接权值都是一样的（即卷积核参数）。而在全连接层中，不同节点的连接权值都是不同的。

- 输入/输出数据的结构化：全连接（将图片展平成一维数组）会破坏图片的空间信息，而卷积运算可以保留图片的空间信息。

## 感受野

对于某层输出特征图上的某个点，在卷积神经网络的原始输入数据上能影响到这个点的取值的区域。通俗来说就是，当前特征图中的像素点，对应到的原图的那块像素区域。

- $3*3$ 的卷积核，步长为 1.

![image-20210405104336376](..\pics\CV\ISG\视觉方向\image-20210405104336376.png)

## 卷积核参数计算

- $(卷积核w*卷积核h*输入通道+偏置)*输出通道$



<img src="../pics/CV/ISG/ERFNet/image-20210312115349713.png" > 
$$
Non-bottlenck：(3*3*w+1)*w+(3*3*w+1)*w = 18w^2 + 2w = 18w_0^2 + 2w_0
$$

$$
Bottlenck：(1*1*w+1)*\frac{w}{4} +(3*3*\frac{w}{4} + 1)*\frac{w}{4} + (1*1\frac{w}{4} + 1)*w = \frac{17w^2}{16}+ \frac{3w}{2} = 17w_0^2+ 6w_0
$$

$$
Non-bt-1D：(3*1*w+1)*w+(1*3*w+1)*w+(3*1*w+1)*w+(1*3*w+1)*w=12w^2+4w=12w_0^2+4w_0
$$

## 卷积核变种

> 普通卷积

<img src="../pics/CV/ISG/视觉方向/ordinary.webp">

一个卷积与所有通道都进行运算。

> 分组卷积（Group Convolution）

<img src="../pics/CV/ISG/视觉方向/group.webp">

将输入特征/图片的通道分成组，每个卷积核也相应地分成组，在对应的组内做卷积。图中分组数，即上面的一组 feature map 只和上面的一组卷积核做卷积，下面的一组 feature map 只和下面的一组卷积核做卷积。每组卷积都生成一个 feature map，共生成两 个feature map。

> 转置卷积/反卷积(Transposed Convolution)

<a href="https://zhuanlan.zhihu.com/p/115070523#:~:text=%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transpose,Convolution%EF%BC%89%EF%BC%8C%E4%B8%80%E4%BA%9B%E5%9C%B0%E6%96%B9%E4%B9%9F%E7%A7%B0%E4%B8%BA%E2%80%9C%E5%8F%8D%E5%8D%B7%E7%A7%AF%E2%80%9D%EF%BC%8C%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E8%A1%A8%E7%A4%BA%E4%B8%BA%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%80%E4%B8%AA%E9%80%86%E5%90%91%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%A0%B9%E6%8D%AE%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A4%A7%E5%B0%8F%E5%92%8C%E8%BE%93%E5%87%BA%E7%9A%84%E5%A4%A7%E5%B0%8F%EF%BC%8C%E6%81%A2%E5%A4%8D%E5%8D%B7%E7%A7%AF%E5%89%8D%E7%9A%84%E5%9B%BE%E5%83%8F%E5%B0%BA%E5%AF%B8%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E6%81%A2%E5%A4%8D%E5%8E%9F%E5%A7%8B%E5%80%BC%E3%80%82">参考文章</a>

- 卷积的反操作。卷积是提取特征，反卷积是还原图片。
- 下面这个示意图就是一种反卷积。把原来 $2*2$ 的图片还原成 $4*4$ 的

<img src="../pics/CV/ISG/视觉方向/no_padding_no_strides_transposed.gif">

> 空洞卷积（Dilated/Atrous Convolution）

是在标准的 convolution map 里注入空洞，以此来增加 reception field。相比原来的正常 convolution，dilated convolution 多了一个 hyper-parameter 称之为 dilation rate 指的是 kernel 的间隔数量(e.g. 正常的 convolution 是 dilatation rate 1)。

- 空洞卷积示例

卷积核没有红点标记位置的为 0，红点标记位置同正常卷积核。

![image-20210405155909354](..\pics\CV\ISG\视觉方向\image-20210405155909354.png)

<img src="..\pics\CV\ISG\视觉方向\no_padding_strides_transposed.gif">

在语义分割（Semantic Segmentation）任务中，一般需要先缩小特征图尺寸，做信息聚合；然后再复原到之前的尺寸，最终返回与原始图像尺寸相同的分割结果图。常见的语义分割模型，如全卷积网络（FCN），一般采用池化操作（pooling）来扩大特征图的感受野，但这同时会降低特征图的分辨率，丢失一些信息（如内部数据结构、空间层级信息等），导致后续的上采样操作（如转置卷积）无法还原一些细节，从而限制最终分割精度的提升。

空洞卷积就是在标准的卷积核中注入“空洞”，以增加卷积核的感受野。空洞卷积引入了扩张率（dilation rate）这个超参数来指定相邻采样点之间的间隔：扩张率为$r$的空洞卷积，卷积核上相邻数据点之间有$r-1$个空洞，如图1.7所示（图中有绿点的方格表示有效的采样点，黄色方格为空洞）。尺寸为$k_w*k_h$的标准卷积核，其所对应的扩张率为$r$的空洞卷积核尺寸为 $k_e+(r-1)(k_e-1),e∈{w，h}$。特别地，扩张率为1的空洞卷积实际上就是普通卷积（没有空洞）。

![image-20210405160645909](..\pics\CV\ISG\视觉方向\image-20210405160645909.png)

> 可变形卷积（DeformableConvolution）

普通的卷积操作是在固定的、规则的网格点上进行数据采样，如图 1.8（a）所示，这束缚了网络的感受野形状，限制了网络对几何形变的适应能力。为了克服这个限制，可变形卷积在卷积核的每个采样点上添加一个可学习的偏移量（offset），让采样点不再局限于规则的网格点，如图 1.8（b）所示。图 1.8（c）和图 1.8（d）是可变形卷积的两个特例：前者在水平方向上对卷积核有较大拉伸，体现了可变形卷积的尺度变换特性；后者则是对卷积核进行旋转。特别地，图 1.8（c）中的可变形卷积核有点类似于上一问中的空洞卷积；实际上，空洞卷积可以看作一种特殊的可变形卷积。

![image-20210405161121034](..\pics\CV\ISG\视觉方向\image-20210405161121034.png)

## 常见CNN结构

### AlexNet

主要网络结构是堆砌的卷积层和池化层，最后在网络末端加上全连接层和 softmax 层以处理多分类任务。

- 采用了 ReLU（修正线性单元，Rectified Linear Unit）作为激活函数，替换了之前的 sigmoid 函数，缓解了深层网络训练时梯度消失问题。
- 引入了局部响应归一化（Local Response Normalization, LRN）模块。
- 应用了 Dropout 和数据扩充（data augmentation）技术来提升训练效果。
- 用分组卷积来突破当时 GPU 的显存瓶颈。

### VGGNet

相比于 AlexNet，VGGNet 做了如下改变。

- 用多个 $3*3$ 小卷积核代替之前的 $5*5$、$7*7$ 等大卷积核，这样可以在更少的参数量、更小的计算量下，获得同样的感受野以及更大的网络深度。
- 用 $2*2$ 池化核代替之前的 $3*3$ 池化核。
- 去掉了局部响应归一化模块。

整体来说，VGGNet 网络结构设计更加简洁，整个网络采用同一种卷积核尺寸（$3*3$）和池化核尺寸（$2*2$），并重复堆叠了很多基础模块，最终的网络深度也达到了近20层。

### inception-v1

在 VGGNet 简单堆砌 $3*3$ 卷积的基础上，Inception 系列网络深入地探索了网络结构的设计原则。

<span style="color:red">参考文献[12]认为，网络性能和表达能力正相关于网络规模，即网络深度和宽度；但过深或过宽的网络会导致参数量非常庞大，这会进一步带来诸如过拟合、梯度消失或爆炸、应用场景受限等问题。</span>一种改进手段是将当前网络中的全连接和卷积等密集连接结构转化为稀疏连接形式，这可以降低计算量，同时维持网络的表达能力。

据此，Inception 系列网络提出了 Inception 模块，它将之前网络中的大通道卷积层替换为由多个小通道卷积层组成的多分支结构。

Inception 模块会同时使用 $1*1$, $3*3$, $5*5$ 的 3 种卷积核进行多路特征提取，这样能使网络稀疏化的同时，增强网络对多尺度特征的适应性。

> Inception-v1 在网络结构设计上的创新。

- 提出了瓶颈（bottleneck）结构，即在计算比较大的卷积层之前，先使用 $1*1$ 卷积对其通道进行压缩以减少计算量（在较大卷积层完成计算之后，根据需要有时候会再次使用 $1*1$ 卷积将其通道数复原），如图1.11（b）所示。
- 从网络中间层拉出多条支线，连接辅助分类器，用于计算损失并进行误差反向传播，以缓解梯度消失问题。
- 修改了之前`VGGNet`等网络在网络末端加入多个全连接层进行分类的做法，转而将第一个全连接层换成全局平均池化层（Global Average Pooling）

![image-20210405162838633](..\pics\CV\ISG\视觉方向\image-20210405162838633.png)

### Inception-v2/v3

Inception-v2/v3 是在同一篇论文里提出的

- 避免表达瓶颈（representational bottleneck），尤其是在网络的前几层。具体来说，将整个网络看作由输入到输出的信息流，我们需要尽量让网络从前到后各个层的信息表征能力逐渐降低，而不能突然剧烈下降或是在中间某些节点出现瓶颈。
- 特征图通道越多，能表达的解耦信息就越多，从而更容易进行局部处理，最终加速网络的训练过程。
- 如果要在特征图上做空间域的聚合操作（如 $3*3$ 卷积），可以在此之前先对特征图的通道进行压缩，这通常不会导致表达能力的损失。
- 在限定总计算量的情况下，网络结构在深度和宽度上需要平衡。

图 1.12 展示了各版本 Inception 模块的结构示意图，图 1.12（a）是Inception-v1 中使用的原始Inception模块；图 1.12（b）、图 1.12（c）、图1.12（d）是 Inception-v2/v3 中使用的、经过卷积分解的 Inception 模块，分别是 Inception-A（将大卷积核分解为小卷积核）、Inception-B（串联[插图]和[插图]卷积）和 Inception-C（并联[插图]和[插图]卷积）。为了缓解单纯使用池化层进行下采样带来的表达瓶颈问题，文中还提出了一种下采样模块：在原始 Inception 模块的基础上略微修改，并将每条支路最后一层的步长改为 2，如图 1.13 所示。

![image-20210405163159733](..\pics\CV\ISG\视觉方向\image-20210405163159733.png)

<div style="text-align: center;">图 1.12 inception 模块及其卷积分解后的变种：Inception A/B/C 模块</div>

----

![image-20210405163407479](..\pics\CV\ISG\视觉方向\image-20210405163407479.png)

<div style="text-align: center;">图 1.13 inception-v2/v3 中的下采样模块</div>

### ResNet

ResNet 的提出源于这样一种现象：随着网络层数的加深，网络的训练误差和测试误差都会上升。这种现象称为网络的退化（degeneration），它与过拟合显然是不同的，因为过拟合的标志之一是训练误差降低而测试误差升高。为解决这个问题，ResNet 采用了跳层连接（shortcut connection），即在网络中构筑多条“近道”，这有以下两点好处。

- 能缩短误差反向传播到各层的路径，有效抑制梯度消失的现象，从而使网络在不断加深时性能不会下降。
- 由于有“近道”的存在，若网络在层数加深时性能退化，则它可以通过控制网络中“近道”和“非近道”的组合比例来退回到之前浅层时的状态，即“近道”具备自我关闭能力。

ResNet 的跳层连接，使得现有网络结构可以进一步加深至百余层甚至千余层，而不用担心训练困难或性能损失。在实际应用中，ResNet-152 模型在ImageNet 2012 数据集的图像分类任务上，单模型能使 Top-5 错误率降至4.49%，采用多模型集成可进一步将错误率降低到 3.57%。

## 批归一化

主要作用是确保网络中的各层，即使参数发生了变化，其输入/输出数据的分布也不能产生较大变化，从而避免发生内部协变量偏移现象。采用批归一化后，深度神经网络的训练过程更加稳定，对初始值不再那么敏感，可以采用较大的学习率来加速收敛。

## 全局平均池化

Network In Network 论文中提出的概念。

论文链接：https://arxiv.org/pdf/1312.4400.pdf，3.2 节。

- 和全连接层相比，使用全局平均池化技术，对于建立特征图和类别之间的关系，是一种更朴素的卷积结构选择。
- 全局平均池化层不需要参数，避免在该层产生过拟合。
- 全局平均池化对空间信息进行求和，对输入的空间变化的鲁棒性更强。

## 激活函数

激活函数决定是否传递信号，即参数是否有效。

深度学习中常用的激活函数是非线性激活函数。非线性激活函数是用于分离非线性可分的数据，是最常用的激活函数。非线性方程控制输入到输出的映射。

常见的非线性激活函数有 Sigmoid、Tanh、ReLU、LReLU、PReLU、Swish 等。

### Sigmoid

是一类 S 型曲线函数。常用的 Sigmoid 型函数有 Logistic 和 Tanh 函数。

以 Logistic 函数为例，分析 Sigmoid 的缺点。
$$
σ = \frac{1}{1+exp(-x)}
$$
对该函数进行求导可的
$$
σ = \frac{e^{-x}}{(1+e^{(-x)})^2}
$$
当 x --> 0 时，导数的值 --> 0，会造成梯度消失。

### ReLU

ReLU（Rectified Linear Unit，修正线性单元）
$$
ReLu(x) = \begin{cases}
x& \text{x≥0}\\
0& \text{x＜0}
\end{cases}
$$
计算高效，采用 ReLU 的神经元只需要进行加、乘和比较操作。且 ReLU 具备很好的稀疏性，大于只有 50% 的神经元会被激活。

且 ReLU 函数为左饱和函数，在 x>0 时导数为 1，在一定程度上可以缓解神经网络梯度消失问题。

### 梯度爆炸

梯度爆炸和梯度消失出现的原因都是因为<b>链式法则</b>。当模型的层数过多的时候，计算梯度的时候就会出现非常多的乘积项，如果式法则不断乘小于（大于）1的数，导致梯度非常小（大）的现象；

> 解决方案

最常见的方案就是更改激活函数，现在神经网络中，除了最后二分类问题的最后一层会用sigmoid之外，每一层的激活函数一般都是用 ReLU。

【ReLU】：如果激活函数的导数是1，那么就没有梯度爆炸问题了。
【好处】：可以发现，ReLU 函数的导数在正数部分，是等于1的，因此就可以避免梯度消失的问题。
【不好】：但是负数部分的导数等于 0，这样意味着，只要在链式法则中某一个zjzj小于0，那么这个神经元的梯度就是 0，不会更新。

【leakyReLU】：在 ReLU 的负数部分，增加了一定的斜率，解决了 ReLU 中会有死神经元的问题。

【ELu】：跟 LeakyReLU 一样是为了解决死神经元问题，但是增加的斜率不是固定的，但是相比 LeakyReLU ，计算量更大。

【正则化】

【batchnorm】

[一文读懂：梯度消失（爆炸）及其解决方法 - 忽逢桃林 - 博客园 (cnblogs.com)](https://www.cnblogs.com/PythonLearner/p/13173560.html)

[了解神经网络中的梯度爆炸 | 机器之心 (jiqizhixin.com)](https://www.jiqizhixin.com/articles/2017-12-21-14#:~:text=梯度爆炸指神经网,重出现重大更新。)

# 计算机视觉

## 物体检测

CV中的基础问题，解决实例分割（Instance Segmentation）、场景理解（Scene Understanding）、目标跟踪（Object Tracking）、图像标注（Image Captioning）等问题的基础。

> 物体检测/目标检测

检测输入图像中是否存在给定类别的物体。若存在，则输出物体在图像中的位置信息，一般这个位置信息是矩形框。

> 分类

- 单步模型
  - 没有独立的、显示的提取候选区域（region proposal），直接由输入图像得到其中存在地物体的类别和位置信息的模型。
  - 典型的单步模型有：OverFeat、SSD、YOLO 系列模型等。
- 两步模型
  - 有独立的、显示的提取候选区域（region proposal）提取过程，即先在输入图像上筛选出一些可能存在的物体的候选区域，然后针对每个候选区域，判断其是否存在物体，若存在，就给出物体的类别和位置修正信息。
  - 典型的两步模型有：R-CNN、SPPNet、Fast R-CNN、Faster R-CNN、R-FCN、Mask R-CNN 等。

- 对比
  - 一般，单步模型速度快，精度比两步模型稍低。

## 语义分割

### `基于编码解码`

> `FCN`

> `UNet`

### `DeepLab系列`

> `DeepLab v1`

- 创新
  - 空洞卷积
  - 全连接条件随机场（CRF）

- 空洞卷积解决了编码过程中信号不断被下采样、细节信息丢失的问题。
- CRF 提高模型捕获局部结构信息的能力。
  - 将每一个像素作为 CRF 的一个节点，像素与像素间的关系作为边，来构造基于全图的 CRF。

> `DeepLab v2`

> `DeepLab v3`

> `DeepLab v3+`

## CNN

### 图像分类

Image Classification

给机械一张图片，决定图片中有什么东西。在做影像类的 DL 的时候，图片输入的大小要固定，把所有图片 scale 成大小一样。

我们的目标是分类，所以我们会把每一个类别表示成一个 One-Hot 向量。

![image-20210418110928640](..\pics\CV\CNN\image-20210418110928640.png)

把图片变成向量，输入到网络。彩色图片是 RGB 三色，三通道的，R、G、B分别表示一个通道。如一张彩色的 $100*100$ 的图片，变成向量后是 $3*100*100$ 的矩阵。（$100*100*3还是3*100*100$要看什么库读的图片）。

图片的每个像素都有一个颜色值，每个颜色值表示这个颜色的强度。

![image-20210418111500521](..\pics\CV\CNN\image-20210418111500521.png)

他是如何识别这是什么类的呢？通过pattern

![image-20210418111837351](..\pics\CV\CNN\image-20210418111837351.png)

看到了尖嘴这个特征，看到了眼睛这个特征，看到了爪子这个特征-->鸟？

`我们并不需要去看整个图片，关注到某些重要的特征即可`。人观察某个物体是什么，一般也是根据经验，去看那些物体的显著特征进行判断。

CNN-->Receptive Field（感受野）

----

同样的模式（特征）会在多个地方出现，我们是不是要为每个地方的同一特征给不同的参数？给的话，参数量未免太大了！

`同一特征，共享参数！`

![image-20210418123818973](..\pics\CV\CNN\image-20210418123818973.png)

虽然共享了参数，但是两个输入的特征不会完全一样，所以输入的参数x是这样的。

![image-20210418124132959](..\pics\CV\CNN\image-20210418124132959.png)

model bias大比较好，model bias小会容易发生过拟合。

### Pooling

> Max Pooling

![image-20210418125100064](..\pics\CV\CNN\image-20210418125100064.png)

选最大的像素值作为代表。为什么要选最大值呢，可能是因为最大值带来的颜色冲击更强烈。

> 典型的CNN结构

![image-20210418125414445](..\pics\CV\CNN\image-20210418125414445.png)

### CNN的缺陷

没法处理影像放大缩小或者是旋转的问题。

# Transformer

## Encoder结构

![image-20210418153846231](..\pics\CV\transformer\image-20210418153846231.png)

输入一排vector，输出的每一排vector是考虑了所有的input以后所得到的结果。

![image-20210418154158406](..\pics\CV\transformer\image-20210418154158406.png)

残差结构

![image-20210418154326892](..\pics\CV\transformer\image-20210418154326892.png)

---

![image-20210418154511420](..\pics\CV\transformer\image-20210418154511420.png)

- Add & Norm
  - Add 就是上图的$a+b$,Norm就是$a+b$后把结果扔进norm层。

这只是原始的transformer的架构，不一定是最好的。

> 论文推荐

- PowerNorm:Rethinking Batch Normalization in Transformers [为什么 Batch Normalization 不如 layer Normalization]

## Decoder结构

### 编码解码整体图

![image-20210418155522364](..\pics\CV\transformer\image-20210418155522364.png)

Decoder 看到的输入，其实是上一个 Decoder 的输出？大致是这个意思。

### Decoder具体结构

![###](..\pics\CV\transformer\image-20210418155848609.png)

### Encoder和Decoder对比

![image-20210418155940154](..\pics\CV\transformer\image-20210418155940154.png)

Decoder第一个模块哪里的Masked的意思是这样的：

- `Self-attention是这样的，每个输出的结果都是考虑了所有的输入。`输出$b^1$的时候其实是根据$a^1 到  a^4$所有的咨询得到的。

![image-20210418160124717](..\pics\CV\transformer\image-20210418160124717.png)

- `Masked self attention`

![image-20210418160425078](..\pics\CV\transformer\image-20210418160425078.png)

产生$b^1$的时候，只考虑$a^1$的咨询

产生$b^2$的时候，考虑$a^1和a^2$的咨询

产生$b^3$的时候，考虑$a^1、a^2、a^3$的咨询

产生$b^4$的时候，考虑$a^1、a^2、a^3、a^4$的咨询

具体一点是这么解释的。

当我们要产生$b^2$的时候，我们只拿第二个位置的query，去跟第一个位置的key和第二个位置的key去计算attention，第三个位置和第四个位置不管它。

![image-20210418160817216](..\pics\CV\transformer\image-20210418160817216.png)

为什么要用Masked？非常的直觉（人的主观判断）

输出是一个一个产生的，所以是先有$a1$再有$a2$和原来的self-attention不一样。原来的self-attention是a1~a4全部都输入到Model中。所以我们根本没法把之后产生的加入进去。

`Decoder的输出是一个一个产生的，它只能考虑它左边的东西。`

Decoder必须自己决定输出的向量长度？如何判断/计算？CV里是故意多设置了几个空对象。NLP里是增加了一个`“断”`来决定程序是需要停止下来的。

![image-20210418161740292](..\pics\CV\transformer\image-20210418161740292.png)

----

![image-20210418161833783](..\pics\CV\transformer\image-20210418161833783.png)

当把“习”输入进去的时候，输出的结果要是一个“断”，那这个解码过程就结束了。

## Encoder与Decoder的桥梁

![image-20210418162514056](..\pics\CV\transformer\image-20210418162514056.png)

- encoder提供两个箭头，decoder提供一个箭头

----

> Cross attention的计算过程

- encoder输出向量$a^1 到 a^4$

- self-attention(mask)得到一个向量，再把这个向量乘上一个矩阵做一个transformer得到q。

- $a^1 到 a^4$产生$k1到k4$

- q和$k^1到k^4$相乘，得到分数$a^、_1 到a^、_4 $

- 接下来再把$a^1 到 a^4$乘上$v1到v3$，在按权重相加得到v，这个v就是接下来会丢到FC的network

q来自于Decoder，k和v来自于Encoder，这个过程叫做Cross Attention

![image-20210418163717465](..\pics\CV\transformer\image-20210418163717465.png)

## Transformer（黑马）

### transformer架构解析

#### 注意力机制

- 什么是注意力：
    - 我们观察事物时，之所以能快速判断一种事物（允许误判）是因为`我们大脑能够很快把注意力放在事物最具有辨识度的部分`从而作出判断，而并非从头到尾的观察一遍事物后，才能有判断结果。正是基于这样的理论，就产生了注意力机制。

---

- 什么时注意力计算规则
    - 需要三个指定的输入Q（query），K（key），V（value），然后通过公式得到注意力的计算结果，这个结果代表query在key和value作用下的表示。这个具体的计算规则多种多样，此处只介绍一种。

---

- 一种注意力计算规则

$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

除以$\sqrt{d_k}$是进行缩放。

----

- Q，K，V的比喻解释

==假如我们有一个问题：给出一段文本，使用一些关键词对它进行描述！==

为了方便统一正确答案， 这道题可能预先已经给大家写出了一些关键词作为提示.其中这些给出的提示就可以看作是key。

而整个的文本信息就相当于是query, value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，

这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，并且能够开始对我们query也就是这段文本，提取关键信息进行表示，这就是注意力作用的过程， 通过这个过程我们最终脑子里的value发生了变化，

根据提示key生成了query的关键词表示方法，也就是另外-种特征表示方法.

刚刚我们说到key和value-般情况下默认是相同，与query是不同的， 这种是我们一般的注意力输入形式，

但有-种特殊情况，就是`我们query与key和value相同， 这种情况我们称为自注意力机制`，就如同我们的刚刚的例子。

使用一般注意力机制，是使用不同于给定文本的关键词表示它，`而自注意力机制，需要用给定文本自身来表达自己`，也就是说你需要从给定文本中抽取关键词来表述它，相当于对文本自身的一-特征提取。

----

- 注意力计算规则的代码分析

```python
def attention(query, key, value, mask=None, dropout=None):
    """
    :param query: 注意力的三个输入张量之一
    :param key: 注意力的三个输入张量之一
    :param value: 注意力的三个输入张量之一
    :param mask:掩码张量
    :param dropout: 传入的Dropout实例化对象
    :return:
    """
    # 首先将query的最后一个维度提取处理啊，代表的是词嵌入的维度。 用于缩放的
    d_k = query.size(-1)

    # 按照注意力计算公式，将query和key的转置进行矩阵乘法，然后除以缩放稀疏。
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)

    # 判断是否使用掩码张量
    if mask is not None:
        # 这步是为了什么？
        # 将掩码张量和0进行位置的意义比较，如果等于0，替换成一个非常小的数
        scores = scores.masked_fill(mask == 0, -1e9)

    # 对scores的最后一个维度进行softmax
    p_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)

    # 最后一步完成p_attn和value张量的乘法，并返回query注意力表示
    return torch.matmul(p_attn, value), p_attn
```

#### multi-head Attention

从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是,我只有使用了一-组线性变化层，即三个变换张量对Q，K, V分别进行线性变换，这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量,他就是每个头都想获得一-组Q， K, V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分， 也就是只分割了最后一维的词嵌入向量.这就是所谓的多头，将每个头的获得的输入送到注意力机制中,就形成多头注意力机制. 

`将词嵌入向量进行切分得到了多头，将每个头的获得的输入送到注意力机制中，形成了多头注意力机制。`

---

- 多头注意力机制的作用：
    - 这种结构设计能力能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明，这种方式可以提升模型效果。

http://jalammar.github.io/illustrated-transformer/

https://zhuanlan.zhihu.com/p/342114940