# 一、SENet

[参考博客](https://blog.csdn.net/u014380165/article/details/78006626)

[github代码](https://github.com/miraclewkf/SENet-PyTorch)

神经网络中，注意力机制通常是一个额外的神经网络，能够硬性选择输入的某些部分，或者给输入的不同部分分配不同的权重。注意力机制能够从大量信息中筛选出重要的信息。

SE-Net（Squeeze-and-Excitation networks)

![image-20210404124745019](..\pics\CV\ISG\注意力机制\image-20210404124745019.png)

$F_{sq}(.)$ 是注意力机制的重要部分，通过它和后面的操作，为每层的 feature 赋予不同的权重（不同的特征有不用的重要性）

> 第一个操作

$F_{sq}(.)$ 压缩，通过全局池化（global pooling）进行压缩，将每个通道的二维特征（$H*W$）压缩为一个实数，论文是通过平均池化的方式实现。这属于空间维度的一种特征压缩，因为这个实数是根据二维特征所有值计算出来的，所以在某种程度上具有全局的感受野，通道数保持不变，所以通过 squeeze 操作后变为 $1*1*C$
$$
z_c = F_{sq}(u_c) = \frac{1}{H*W}\sum_{i=1}^{H}\sum_{j=1}^{W}u_c(i,j)
$$
$u_c$ 中的 c 应该是指的通道。计算 c 通道的 squeeze 后的值。

> 第二个操作：excitation

通过参数来为每个特征通道生成一个权重值，这个权重值是如何生成很关键！论文是通过两个全连接层组成一个Bottleneck结构去建模通道间的相关性，并输入和特征输入同样数目的权重值。

> 第三个操作：Scale

将前面得到的归一化权重加权到每个通道的特征上。论文中的方法是用乘法，逐通道乘以权重系数，完成再通道维度上引入 attention 机制。

![image-20210404131134167](.\..\pics\CV\ISG\注意力机制\image-20210404131134167.png)

SE block 嵌入到主流网络（Inception为例）中的。

Global pooling就是squeeze操作，$FC+Relu+FC+sigmoid$ 就是excitation操作，具体过程是首先通过一个$FC$将特征维度降低到原来的$\frac{1}{r}$，然后经过$Relu$函数激活后再通过一个$FC$升回到原来的特征维度C，然后通过sigmoid函数转化为一个0~1的归一化权重。这里FC层的参数根据loss一起不断迭代更新。

> 补充说明与思考

（1）SEnet的核心思想在于通过全连接网络根据loss自动学习特征权重（而不是直接根据特征通道的数值分布来判断），使得有效的特征通道权重大。增加了参数和计算量，但是效果更好了。

（2）论文认为在excitation操作中用两个FC比直接用一个FC的好处在于：

- 具有更多的非线性。
- 可以更好地拟合通道间的复杂度。