# 个人看法

未提出新的网络结构，而是依靠一些技巧，讲分割任务进行统一。

对网络结构进行了微小的调整：

- `ReLU`换`leaky ReLU`
- `Batch Norm` 换 `Instance Norm`

只记录个人认为值得参考的部分

# 论文内容

## 摘要

提出一种鲁棒的基于`2D UNet`和`3D UNet`的自适应框架`nnUNet`。作者在各种任务上拿这个框架和目前的SOTA（state of the art）方法进行了比较，且该方法不需要手动调参。最终`nnUNet`得到了最高的平均dice。

## 介绍

提出问题：医疗图像分割被CNN主导，不同的任务需要不同的结构和不同的调参策略才能达到各自的最佳效果。

是否能有一种通用的方法？不进行人为调整就达到在各个数据集上都有很好的效果？

作者认为过多的人为调整网络结构，会导致对于特定数据集的过拟合。非网络结构方面的影响可能对于分割任务影响更大。

作者提出一种nnUNet（no-new-Net）框架，基于原始的UNet（很小的修改），不去采用哪些新的结构，如相残差连接、dense连接、注意力机制等花里胡哨的东西。相反的，把重心放在：预处理（resampling和normalization）、训练（loss，optimizer设置、数据增广）、推理（patch-based策略、test-time-augmentations集成和模型集成等）、后处理（如增强单连通域等）。

## 方法

### 网络结构

用基础的`UNet`结构：`2D UNet，3D UNet，UNet`级联

微小修改

- `ReLU`换`leaky ReLU`
- `Batch Norm` 换 `Instance Norm`

### 预处理

整体数据Crop：只在非零区域内crop，减少计算消耗

### 训练过程

从头训练，使用五折交叉验证，loss函数：结合dice loss和交叉熵loss；

$L_{total} = L_{dice} + L_{CE}$

对每一个样本单独计算dice loss，然后再batch上平均。做平均大概就是为了防止极端情况。

### 其他训练参数：

> Adam优化器，学习率3e-4；250个batch/epoch；
> 学习率调整策略：计算训练集和验证集的指数移动平均loss，如果训练集的指数移动平均loss在30个epoch内减少不够5e-3，则学习率衰减5倍；
> 训练停止条件：当验证集指数移动平均loss在60个epoch内减少不够5e-3，或者学习率小于1e-6，则停止训练。

**数据增广**：随机旋转、随机缩放、随机弹性变换、伽马校正、镜像。

**patch采样：**为了增加网络的稳定性，patch采样的时候会保证一个batch的样本中有超过1/3的像素是前景类的像素。这个很关键，否则你的前景dice会收敛的很慢。【在细胞分割的测评中，我采用的也是尽量截取可能多的像素，避免某些类别数量太少】