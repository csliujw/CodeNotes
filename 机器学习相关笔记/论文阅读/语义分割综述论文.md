#  论文名称

深度卷积神经网络图像语义分割研究进展_青晨

# 我希望从中得到什么？

常见的语义分割网络，网络是如何一步一步改进的，那些隐藏的问题是如何凸显出来的，又是如何解决的，有没有什么相似之处。

==基础知识，监督语义分割可以多看几次。==

#  摘要

论文摘要：扼要说明研究工作的目的、研究方法和最终结论。

本文首先分析和描述了语义分割 领域存在的困难和挑战，介绍了语义分割算法性能评价的常用数据集和客观评测指标。然后，归纳和总结了现阶 段主流的基于深度卷积神经网络的图像语义分割方法的国内外研究现状。

要学习的地方：

- 困难和挑战（未来的突破点；目标，类别，背景）
- 评价指标（最终目的，提高指标）
- 从现状中发现问题（黑盒，数学不漂亮；计算时间大；不容易收敛；）

# 引言

语义分割是像素级的图像理解，对图像中的每一个像素标注所属类别。

传统图像分割算法的分割策略。

- 通过图像的颜色、纹理信息、空间结构等特征，将其分割为不同的区域。同一区域内具有一致的语义信息。

传统图像的分割方法

-  阈值分割
- 区域生长
- 边缘检测
- 图划分
- 归一化分割（经典分割方法）
- `GrabCut`（经典分割方法）

提出深度学习概念后，利用多层神经网络从大量训练数据中自动学习高层特征。对比传统手工设计的特征，`DCNN`学习的特征更加丰富、表达能力更强。利用`DCNN`可以实现端到端的语义分割预测。

语义分割难点

- 目标：不同光照、角度、距离等条件下，拍摄的图像会明显不同。
- 类别：同类目标之间存在相异性，而不同类目标之间存在相似性的问题。
- 背景：实例场景中的背景是复杂的，不利于实现语义分割。

该综述将

- 详细描述了每种方法的创新工作并分析了**存在的问题**。
- 介绍语义分割存在的**问题与挑战**。
- 语义分割算法性能评价的常用数据集和客观评测指标。
- 指出了语义分割领域**未来的研究方向**。

#   常用数据集与评测指标

## 常用数据集

- `PASCAL VOC` 
- `MS COCO( microsoft common objects in context)`
- `KITTI ( Karlsruhe Institute of Technology and Toyota Technological Institute)`
- `PASCAL-Part`
- `Cityscapes`
- `CamVid( Cambridge-driving labeled video database)` 
- etc....

## 客观评测指标

- 运行时间

- 显存占用

- 准确率

  - 像素准确率 pixel accuracy，PA 分类正确的像素占总像素的比例。但是，不同类别的样本数量差别很大的情况时，像素准确率并不能客观反映模型性能。
  - 平均像素准确率( mean pixel accuracy，`MPA`)
  - **平均交并比**( mean intersection-over-union，`MIoU`) ==最重要！更能反映模型的准确程度==

  假设一共有 c + 1 个类别( 包括一个背景类) ，记 `pij` 是将 i 类预测为 j 类的像素数，换句话说，`pii` 表示 正确预测的正样本的像素数( true positives) ; `pij` 和 `pji` 分别表示错误预测的正样本的像素数( false positives) 和错误预测的负样本的像素数( false negatives) 。**像素准确率是分类正确的像素占总像素的比例**（如果每个类别之间的数量差距大，但是数量多的类别识别率高，数量上的类别识别率低，这种判断方式是不合理的！），可表示为：

  $P = \frac{\sum_{i=0}^{c} P_{ii}}{\sum_{i=0}^{c}\sum_{j=0}^c P_{ij} }$

  平均像素准确率计算**每个类内准确分类像素数的比例**，**再求所有类的平均**，可表示为：

  $P = \frac{1}{c+1} \sum_{i=0}^c \frac{P_{ii}}{\sum_{j=0}^c P_{ij}}$

  **==平均交并比是语义分割性能的标准度量值==**，计算正确预测的正样本( 真实值与预测值这两个集合的交集) 与正确预测的正样本、错误预测的正样本 和错误预测的负样本之和( 并集) 的比值，可表示为：

  $R_{MIou} = \frac{1}{c+1} \sum_{i=0}^c \frac{P_{ii}}{\sum_{j=0,j \neq i} (P_{ij}+P_{ji})+P_{ii}}$

#  基于监督学习的语义分割模型

需要预先标注训 练图像的每一个像素，再利用标注好的数据集训练 语义分割网络。[ 这工作量很大啊。。。 ]

## 大致分类

- **基于解码器的方法**  [ `FCN` `U-Net` ，上采样还原图像，还原的图不清晰 ]
- **基于特征图的方法** [ `DeepLab v3`,改进了空洞空间金字塔池化,用不同扩展率的多个并行的空洞卷积层实现多尺度处理 ]
- 基于概率图的方法 [  ]
- 多种策略结合的方法

## 基于解码器的方法

**编码**：为图像生成低分辨率的特征图

**解码**：通过上采样将低分辨率的特征图映射到原图像尺寸，产生像素级的语义标签。[ 这样上采用得到的图像还是存在相对严重的问题 ]

解码器的输出是一个表示 图像类别标签( class label) 的矩阵，矩阵中每一个元 素的值与像素所属的类别相对应。

2015年提出的`FCN`，将传统的CNN最后的3个连接层替换为卷积层，输出一个类别标记矩阵，实现像素到像素的映射。

### **`FCN`**

由于全卷积网络不使用全连接层，因此不受全连接层参数的限制（不是很理解？是计算速度不手全连接参数的影响吗？可以不太考虑图片尺寸带来的无法收敛的问题？绝大多数的参数好像都是在全连接层！==讨论==），可以输入任意尺寸的图像。

**`FCN`的做法**

为了恢复特征图的空间维度，全卷积网络对每层的特征图利用==双线性插值进行上采样==，然后逐像素分类，生成与输入图像尺寸相 同的分割结果。由于池化操作造成的信息损失，通 过简单的上采样只能获得粗略的分割结果，因此该 方法融合了多分辨率的信息，即对不同尺寸的特征 图分别上采样后进行特征融合，从而获得更准确的 分割结果。

`FCN`的具体做法是 全连接变为了卷积，在这3个卷积层，保留了每层的特征。同时作者还尝试了把最前面几层的特征图保留融合，但是效果并不理想。

**`FCN`的不足**

虽然全卷积网络融合了多分辨率信息， 但是通过简单的双线性插值得到的分割结果仍较为模糊和粗糙，无法完整地还原图像中的细节信息。

### U-Net

具有对称编码器-解码器结构的U-Net

对`FCN`结构进行改进。编码器部分与`FCN`相似，通过卷积与池化操作提取特征，而在解码器部分中，U-Net 将编码器每一层输出的特征图与对应的解码器生成的特征图相融合，即将深层语义特征与细粒度的浅层细节特征 相结合，从而生成更准确的分割图像。

这样看`FCN`和`UNet`的区别似乎没那么明显！

- U-net与其他常见的分割网络有一点非常不同的地方：U-net采用了完全不同的特征融合方式：拼接，U-net采用将特征在channel维度拼接在一起，形成更厚的特征。而`FCN`融合时使用的对应点相加，并不形成更厚的特征。
- `FCN`式的对应点相加，对应于`TensorFlow`中的`tf.add()`函数；
  U-net式的channel维度拼接融合，对应于`TensorFlow`的`tf.concat()`函数，比较占显存。

<a href="https://blog.csdn.net/weixin_41108334/article/details/87917748">对应博客</a>

### 全卷积网络的问题

- 网络预定义了感受域( receptive field) 的大小，所以*大于或者小于感受域的目标，可能被分裂或者错误标记*; 
- 二是由于上采样操作( 双 线性插值) 过于简单，使得目标的细节信息丢失或者被平滑处理。

### 解决

提出了一种对称的语义分割网络模型`DeconvNet`。

编码器部分：采用`VGG16`

解码器部分：由多层转置卷积网络代替简单的上采样操 作，生成比全卷积网络更准确的分割图。

多层转置 卷积网络包括上池化层( `unpooling layers`) 和转置卷 积层( deconvolution layers) 。在池化过程中利用切 换变量( `switch variables`) 记录最大池化操作确定的最大激活值的位置，然后上池化层利用转换变量将 最大激活值映射回原来的位置，从而恢复图像的空 间分辨率，其他像素值用 0 填充，生成稀疏的特征图。（<u>记录最大池化的位置，再还原回原来的位置，其他地方填充为0</u>）。再用转置卷积转换为稠密图。

### `SegNet`

好像被`Resnet`碾压了。。

种旨在应用于智能驾驶和机器人领域的语义分割模型，主要以道路场景理解为动机，具有对外观(道路、建筑物) 和形状( 汽车、行人) 建模的能力，能够学习不同类别(如道路和人行道)之间的上下文关系。

`SegNet`去掉了全连接层，减少了参与训练的参数数量，从而提高了预测结果的效率。`SegNet` 增加了批归一化操作，加快了网络的收敛速 度并抑制了过拟合现象。

目标边界的分割 精度仍然有待提高。

## 基于特征图的方法

### 膨胀卷积

结合特征图的上下文信息：

- 空间上下文（位置）不理解 [ 空间上下文，增大感受域？] 
- 尺度上下文（尺寸）

这类方法通过增大感受域和融合多尺度可以获得空间上下文和尺度上下文，可有效提升网络的分割性能。

以 `FCN` 和 `SegNet` 为代表的语义分割模型都利用卷积结合池化操作提取特征。池化的目的是缩小 图像尺寸、降低计算量和避免过拟合。由于语义分 割需要得到像素级的预测，所以需要进行上采样操 作，使输出图像的尺寸与输入图像的尺寸保持一致。 经过了下采样和上采样操作，图像丢失了大部分信 息，使得语义分割的精度下降。

用膨胀卷积来代替池化层和上采样层。

dilated convolution（膨胀卷积、空洞卷积）：通过再卷积核之间填充固定数量的0元素，达到在不增加卷积核参数数量的情况下使用卷积核的感受域增大。

<img src="..\..\pics\pytorch\dilated_convolution.gif" style="float:left">

膨胀卷积，卷积核不连续，损失了图像连续性信息，不利于小目标的分割。

<a href="https://blog.csdn.net/qq_30241709/article/details/88080367">知识有限，暂时靠博客续命</a>

### `DeepLab v3`

改进了空洞空间金字塔池化（`atrous` spatial pyramid pooling，==**`ASPP`**==）模型。，使用不同扩张率的多个并行的空洞卷积层实现多尺度处理。

空洞卷积的计算方式

<img src="..\..\pics\pytorch\dilation_rate.jpg">

`DeepLab v3`采用不同的扩张率，用多个并行的空洞卷积层实现多尺度处理，并添加了全局平均池化。

==`DeepLab v3`示意图==

<img src="..\..\pics\pytorch\DeepLab_v3.png">

`DeepLab v1 v2`加入了v一个条件随机场，那个好像优化起来很麻烦，效果也不好，略过？？

### `RefineNet`

常见的网络利用的是深层+浅层信息的提取信息。

==`RefineNet` 深层特征图 + 浅层特征图 + 下采样中丢失的信息（`如何获得下采样丢失的信息？`）==

`RefineNet`模块由残差卷积单元（residual convolution unit），多分辨率融合（multi-resolution fusion）和链式残差池化（chained residual pooling）组成。

`RefineNet` 与 `DeepLab v3` 均 利用了多尺度的方法，不同之处在于 `DeepLab v3` 是 对输入图像进行多尺度的空洞卷积后进行特征融 合，而 `RefineNet` 是对不同尺寸的输入图像进行特征 提取后融合。【`FCN`也尝试过把每层的特征提取后融合，但是效果并不理想（疑问  ）】

### `FoveaNet`

通用图像解析模型的问题：不同位置、尺寸的图像采用的都是同样的解析模型，忽略了图像的几何特性，难以解析远处小尺寸和近处大尺寸的目标。

`FoveaNet`一定程度上解决了这种问题。

`FoveaNet`由透视引导解析网络 ( perspective-aware parsing network) 和透视引导条件 随机场( perspective-aware `CRF`) 组成，对不同尺寸的目标采用不同的解析方法。 `DeepLab v1 v2`也用了`CRF`【不好改进】

透视引导条件随机场

- 条件随机场 好像是可以增大感受野。

- 解决了近处大尺寸目标区域不完整的 问题，结合透视图和目标检测技术，使属于近处目标 的像素有更大的势能，属于远处目标的像素有更小 的势能，有效缓解了目标区域不完整和过度平滑的问题。

多尺度信息可以提高语义分割的进度。U-Net的对称结构 `DeepLab`系列中的改进金字塔空洞卷积 都是采用了多尺度信息，增加准确度。

多尺度的感受域可以学习不同尺寸目标的信息。

- 全局的场景分类能 够提供图像语义分割的类别分布信息
- 金字塔池化 模型( pyramid pooling module) 通过使用较大卷积核 的池化层获取类别分布信息

<img src="..\..\pics\pytorch\FoveaNet.png">

### `PSPNet`

全局场景信息的空间金字塔池化网络

- 利用CNN模型提取输入图像的特征
- 将特征图送入金字塔池化模型
- 融合四种并行的不同尺度的池化特征

空间金字塔池化模型融合了局部和全局信息，利用不同的空间信息来对场景进行整体理解。

---

## 基于概率图的方法

结合语义上下文（概率）和空间上下文（位置）信息。

像素与像素之间用概率进行关联，建立像素之间的语义关系，细化分割目标的边界，提高分割精度（这块，引入了概率，我感觉，，之前也有网络引入了概率，但是分割效果，，）。常用的概率图模型有`CRF`、`MRF`（马尔可夫随机场）和贝叶斯网络。

没看太懂。

## 多种策略结合

基于**解码器**的方法通过**上采样**将分类结果**映射到原图像尺寸**，产生像素级别的语义标签，从而获得 语义分割的结果。

基于**特征图**的方法通过**空洞卷 积、空间金字塔池化和多尺度空洞卷积**等**获得**图像**多尺度的感受域**，有助于**提升**网络的**分割性能**。

基于**概率图**的方法通过**对像素的类别概率进行分析**， 以**结构化的方式细化分割目标的边界**，提高目标分割的精度。多种策略结合的方法是将上述多种策略 相结合，显著提高了网络的分割性能。

# Mask R-CNN

- `RoIAlign`（region proposal networks，`RPN`）候选目标的边界框
- <a href="https://blog.csdn.net/u011918382/article/details/79455407">RoIAlign</a>

`FCN`对每个`RoI`进行分割。以像素到像素的方式预测分割掩码。

Mask R-CNN模型不仅分割精度更高，而且模型更加灵活，可以用来完成多种计算 机视觉任务，包括目标分类、目标检测、实例分割、 人体姿态识别等。在训练阶段，Mask R-CNN 模型 使用多任务损失约束 L，其表达式如下：$L = L_{cls} + L_{box} + L_{mask} $ 

其中，$L_{cls}$ 表示目标分类的损失，$L_{box}$ 为检测任务的 损失，$L_{mask}$ 是实例分割损失。

实例分割评价指标，像素精度（pixel accuracy，PA）表示分类正确的像素占总像素的比例。

$PA = \frac{\sum_{i=0}^{k}P_{ij}}{\sum_{i=0}^{k}\sum_{j=0}^{k}P_{ij}}$





















