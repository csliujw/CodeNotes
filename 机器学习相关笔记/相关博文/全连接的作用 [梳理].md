# 文章来源

<a href="https://www.cnblogs.com/Terrypython/p/11147665.html">文章来源</a>

## 1 作用

众所周知，全连接层之前的作用是提取特征，全连接层的作用是分类。

## 2 关键阶段介绍

假设通过CNN已经提取到特征，下一层是全连接层，这个阶段比较关键，举个例子说明：

![img](https://img2018.cnblogs.com/blog/1414369/201907/1414369-20190707202452838-1402524928.jpg)

上图中CNN的输出是3x3x5的特征图，它是怎么样转换成1x4096的形式呢？

很简单,可以理解为在中间做了一个卷积。这一步卷积一个非常重要，它的作用就是把**分布式特征representation映射到样本标记空间！**

简单来说：它把特征representation整合到一起，输出为一个值。这样做,有一个什么好处：就是大大减少特征位置对分类带来的影响。

注：样本标记空间也叫样本输出空间。

再举个例子说明上述的过程：

![img](https://img2018.cnblogs.com/blog/1414369/201907/1414369-20190707203045490-1770593040.jpg)

从上图我们可以看出，猫在不同的位置，输出的feature值相同，但是位置不同。对于电脑来说，那分类结果也可能不一样。

而这时全连接层filter的作用就相当于：猫在哪我不管，只要找到猫就可以，于是让全连接层filter去把这个猫找到，实际就是把feature map 整合成一个值。

这个值大，有猫，这个值小，那就可能没猫，这和猫在哪关系不大了，鲁棒性大大增强了！

因为空间结构特性被忽略了，所以全连接层不适合用于在方位上找模式的任务，比如分割。

<span style="color:green">**这个例子好特殊**</span>

## 3 全连接层为什么大部分是两层

只用一层fully connected layer 有时候没法解决非线性问题，而如果有两层或以上fully connected layer就可以很好地解决非线性问题了。

## 4 换个方式讲

现在的任务是去识别一图片是不是猫。

![img](https://img2018.cnblogs.com/blog/1414369/201907/1414369-20190707202132933-1905450358.jpg)

假设这个神经网络模型已经训练完了，全连接层已经知道

![img](https://img2018.cnblogs.com/blog/1414369/201907/1414369-20190707204536772-2143710072.jpg)

当我们得到以上特征，我就可以判断这个是不是猫了，因为全连接层的作用主要就是实现分类（Classification）。

![img](https://img2018.cnblogs.com/blog/1414369/201907/1414369-20190707204651241-1957455207.jpg)

从上图，可以看出：

**红色的神经元表示这个特征被找到了（激活了），同一层的其他神经元，要么猫的特征不明显，要么没找到。**

当我们把这些找到的特征组合在一起，发现最符合要求的是猫，则认为这是猫了！