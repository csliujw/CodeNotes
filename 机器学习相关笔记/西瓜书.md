# SVM

Support vector machine

对于样本量小的，用SVM往往可以得到很好的效果。但处理大数据集，SVM不是很合适。【问题：什么规模的算小，什么规模的算大】

## 线性模型

线性可分，可通过一条直线进行划分。

将直线两端平移，直到插到了要划分的那些向量，那些被插入的向量叫支持向量。被平移的线到原线的距离叫间隔，svm要找的是那个最小间隔。

问题逐渐变为 求最小间隔（线与线的平行距离）

在求解的时候对方程进行了放缩，以简化求解。

<span style="color:red">笔记阅读流程：P121-P123顺序观看</span>

## 非线性模型

非线性模型，不可以通过一条直线进行划分。

为此，我们把当前的低维空间上升为高维空间，在高维空间中用直线进行划分。

一个说法：维度越高，被线性分开的概率越大。如果是无限维度，那么他线性可分的概论将会是1，西瓜书P126笔记（非线性映射）

我们不知道怎么把低维映射到高维【无限维映射的显示表达式】，我们需要只要知道一个核函数（Kernal Function），用核函数替代显示表达式（这个表达式维数可能很高，故采用核函数）。

- 核函数：替代了低维映射到高维的那个函数fai。
- 用核函数替代fai，简化优化问题

之后补充了下最优化论的知识。

为什么要引入原问题 ， 对偶问题， 强对偶定理？

目的是为了只用K不用fai解决那个问题。

用原问题解决对偶问题。用原问题的方式解决对偶问题。

视频：胡浩基，机器学习11

----

**思路整理：**

非线性如何划分？

将低维-----映射----->高维

通过核函数将低维映射到高维（维度越高，线性可分的概率越大）

如何用核函数取代fai。

- 核函数取代fai的充要条件（A4纸笔记）
  - K( X1 , X2 ) = K( X2 , X1 )
  - 半正定
- 途中引入了优化理论的内容
  - 在特定条件，可以用 通过原问题的方式解决对偶问题（证明见A4纸笔记）
  - 引出了原问题和对偶问题，在特定条件下，原问题和对偶问题的解是一致的。我们把对原问题的求解化为了对偶问题的求解。这样我们只要求对偶问题的解就可以了。<span style="color:red">统计学习方法中有详细推导说明</span>

<span style='color:red'>笔记阅读流程：130->126->127->123对偶问题</span>

但是有个核函数可以完成这个工作。

我们可以不知道 

后向传播算法。

从后向前算偏导。先计算连接最为丰富的那些参数的偏导（如b和w1，w2，b1有关，我们先计算偏E/偏b  【E是损失函数（预测值和真实值之间的差距）】）这样做的好处好像是减少了直接一个一个求偏导的计算量。









