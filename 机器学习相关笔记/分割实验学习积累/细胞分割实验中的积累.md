# 理论和方法层面

- U-Net 为了分割细胞的边界，引入了权重
- 结合了局部信息和全局信息，U型结构
- 无法还原图片大小

## 调参

根据训练效果进行调整

- 学习率
- 步长
- 网络层数

## 常用损失函数

损失函数用平方还是非平方。

## 常用优化器





#  数据的一些基本处理

## 文件重命名

细胞的数据集中，一个图片中有多个符合要求的细胞，label是把符合要求的细胞是一个一个保存的。现在我想用一个图片其中的一个label图片进行训练，需要对文件进行重命名操作。

> <span style="color:green">**注意：**a.txt ; a.jpg这些是文件，目录他也是文件！</span>

**这只是提供一个最基础的模板，供未来2021年入学的编程和我一样菜的学弟学妹们参考**

```python
import os

# 基础目录
BASE_DIR = '/home/kkx/train/y/'

def func(path):
    # 遍历改目录中的所有文件（目录也是文件！）
    for i in os.listdir(path):
        # 拼接绝对路径
        path2 = os.path.join(path, i)  
         # 判断如果是文件夹,递归遍历  改文件夹
        if os.path.isdir(path2): 
            func(path2)
        # 是文件的话就重命名
        else:
            # 我的重命名是把后缀改了，这种写法不严谨，请勿模仿
            new_name = i.replace("gif", "jpg")
            # 文件重命名操作
            os.rename(BASE_DIR+i, BASE_DIR+new_name)

def func2(path):
    for i in os.listdir(path):
        path2 = os.path.join(path, i) # 拼接绝对路径
        if os.path.isdir(path2): # 判断如果是文件夹,调用本身
            func2(path2)
        else:
            # 获得后缀【我的文件名是123_23.bmp 我想把下划线后面的删除了，如123_23.bmp变成 123.bmp】
            end = i[-3:] # 取到后缀
            name = i.split('_')[0] # 获得文件名称
            new_name = name+'.'+end # 拼接新的name
            # 重命名
            os.rename(BASE_DIR+i,BASE_DIR+new_name)

if __name__ == "__main__":
    func2(BASE_DIR)

```

## `OpenCV`访问图片

```python
"""
找到文件，前缀相同的放在一个list里。
"""
import os
import numpy as np
import cv2 as cv
import logging

# 细胞壁
CELL_WALL = [20, 20, 20]
# 细胞核
CELL_KERNEL = [40, 40, 40]
# 背景色
CELL_BACKWORD = [0, 0, 0]
# 图片后缀名
IMAGE_SUFFIX = '.bmp'

logging.basicConfig(level=logging.DEBUG)

"""
修改下颜色复制的数值试一下，看看最后是不是单通道的元素。
"""
class process():

    def __init__(self, source_dir, target_dir):
        self.source_dir = source_dir
        self.target_dir = target_dir

    """获得文件的原始名,返回的数据格式为字典类型 {"file_name":[]}"""

    def get_primeval_name(self):
        file_name = os.listdir(self.source_dir)
        all_name = []
        for tmp in file_name:
            all_name.append(tmp.split("_")[0])
        # set去重
        singal_name = list(set(all_name))
        name_dict = {}
        for i in singal_name:
            name_dict[i] = []
        return name_dict

    """
    找到原始图片和名称一一对应的那些图
    return {  '1231':['123_1.jpg' , '123_2.jpg']  }
    """

    def find_suite_img(self):
        file_name = os.listdir(self.source_dir)
        name_dict = self.get_primeval_name()
        for i in file_name:
            primeval_name = i.split("_")[0]
            if primeval_name in name_dict.keys():
                name_dict[primeval_name].append(i)
        return name_dict

    """
    合并图片
    """

    def merge_image(self):
        name_dict = self.find_suite_img()
        logging.info("total image count is {}".format(len(name_dict)))
        for key in name_dict.keys():
            logging.info("current key is {}".format(key))
            item_list = name_dict[key]
            """合并同一系列的图片 如 101_1.bmp 101_2.bmp"""
            self._merge_image(key, item_list)

    """合并同一原图的 label"""

    def _merge_image(self, key, item_list):
        image_obj = []
        for name in item_list:
            image_obj.append(cv.imread(self.source_dir + name))
        save_image = self.generate(image_obj[0])
        H, W = image_obj[0].shape[:2]
        # 能不能改成列表推导式
        logging.info("the key is {}".format(key))
        for h in range(H):
            for w in range(W):
                # 从所有的label的h w位置中选出最合适的像素点
                bgr = self.choose_best_pixel(h, w, image_obj)
                if bgr != [0, 0, 0]:
                    save_image[h, w] = bgr
        cv.imwrite(self.target_dir + key + IMAGE_SUFFIX, save_image)
        logging.info("save image {}".format(self.target_dir + key + IMAGE_SUFFIX))

    """
    *args : 像素值的元组，选择一个最好的 默认20 20 20最好
    暂时这样计算  20 20 20 是给了三通道吗？ 给20就好了
    """

    def choose_best_pixel(self, h, w, image_obj):
        cell_kernel = False
        cell_wall = False
        for tmp in image_obj:
            if (tmp[h, w] == CELL_WALL).all():
                cell_wall = True
            elif (tmp[h, w] == CELL_KERNEL).all():
                cell_kernel = True
        if cell_kernel:
            return CELL_KERNEL
        if cell_wall:
            return CELL_WALL
        return CELL_BACKWORD

    """
    生成一张空白图
    image_obj 
    """

    def generate(self, image_obj):
        obj = np.zeros(image_obj.shape, dtype=np.int8)
        H, W = image_obj.shape[:2]
        obj[:, :] = (0, 0, 0)
        return obj

if __name__ == "__main__":
    source_dir = '/home/qq/code/Pytorch-UNet/cell_data/patch_data/train/mask/'
    target_dit = '/home/qq/code/Pytorch-UNet/cell_data/patch_data/train/mask_copy/'
    p = process(source_dir=source_dir, target_dir=target_dit)
    p.merge_image()
```



## 训练过程中出现错误

原始的label是单通道 `8bit`的，我合成处理后的是24通道的图，所以`unet`训练报错了。

解决办法：

- `cv`分割出三个通道，然后将其中的一个通道保存为图片

  ```python
  """大致就这意思"""
  import cv2 as cv
  if __name__ == "__main__":
      obj = cv.imread("image_path")
      b,g,r = cv.split(obj)
      cv.write("image_path",b)
  ```

- <span  style="color:green">**PS：以后处理完数据后，要看下处理后的数据与原来的label在重要参数方面是否保持一致**</span>

# 语法和`API`层面的

## assert

```python
assert listA == listB, 'if A not equals B, string will print'
```

## logging

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logging.info("safd")
```

## `argparse`

```python
import argparse

def get_args():
    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=500,
                        help='Number of epochs', dest='epochs')
    parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=1,
                        help='Batch size', dest='batchsize')
    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.0001,
                        help='Learning rate', dest='lr')
    parser.add_argument('-f', '--load', dest='load', type=str, default=False,
                        help='Load model from a .pth file')
    parser.add_argument('-s', '--scale', dest='scale', type=float, default=0.5,
                        help='Downscaling factor of the images')
    parser.add_argument('-v', '--validation', dest='val', type=float, default=10.0,
                        help='Percent of the data that is used as validation (0-100)')
    return parser.parse_args()
```





# 代码编写层面

## `OpenCV`预处理图像

### U-Net网络模型

```python
import torch
import torch.nn as nn
from torch import optim
from torch.nn.modules import loss

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")


class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.conv(x)


class UNet(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(UNet, self).__init__()
        self.conv1 = DoubleConv(in_ch, 64)
        self.pool1 = nn.MaxPool2d(2)  # 每次把图像尺寸缩小一半
        self.conv2 = DoubleConv(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.conv3 = DoubleConv(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.conv4 = DoubleConv(256, 512)
        self.pool4 = nn.MaxPool2d(2)
        self.conv5 = DoubleConv(512, 1024)
        # 逆卷积
        self.up6 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        # conv6层  有512个通道是u形左边裁剪过来的，所以是1024
        self.conv6 = DoubleConv(1024, 512)
        self.up7 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.conv7 = DoubleConv(512, 256)
        self.up8 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.conv8 = DoubleConv(256, 128)
        self.up9 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.conv9 = DoubleConv(128, 64)

        self.conv10 = nn.Conv2d(64, out_ch, 1)

    def forward(self, x):
        c1 = self.conv1(x)
        p1 = self.pool1(c1)
        c2 = self.conv2(p1)
        p2 = self.pool2(c2)
        c3 = self.conv3(p2)
        p3 = self.pool3(c3)
        c4 = self.conv4(p3)
        p4 = self.pool4(c4)
        c5 = self.conv5(p4)

        up_6 = self.up6(c5)
        c4_new = c4[:, :, 0:up_6.shape[2], 0:up_6.shape[3]]
        merge6 = torch.cat([up_6, c4_new], dim=1)  # 按维数1（列）拼接,列增加
        c6 = self.conv6(merge6)

        up_7 = self.up7(c6)
        c3_new = c3[:, :, 0:up_7.shape[2], 0:up_7.shape[3]]
        merge7 = torch.cat([up_7, c3_new], dim=1)
        c7 = self.conv7(merge7)

        up_8 = self.up8(c7)
        c2_new = c2[:, :, 0:up_8.shape[2], 0:up_8.shape[3]]
        merge8 = torch.cat([up_8, c2_new], dim=1)
        c8 = self.conv8(merge8)

        up_9 = self.up9(c8)
        c1_new = c1[:, :, 0:up_9.shape[2], 0:up_9.shape[3]]
        # 
        merge9 = torch.cat([up_9, c1_new], dim=1)
        c9 = self.conv9(merge9)

        c10 = self.conv10(c9)

        out = nn.Sigmoid()(c10)
        return out

```

### 网上看到的`UNet`模型代码采用了`CRF`

```python
""" Parts of the U-Net model """

import torch
import torch.nn as nn
import torch.nn.functional as F


class DoubleConv(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)


class Down(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class Up(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()

        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)


    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)
```

----

```python
""" Full assembly of the parts to form the complete network """

import torch.nn.functional as F

from .unet_parts import *


class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=True):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        factor = 2 if bilinear else 1
        self.down4 = Down(512, 1024 // factor)
        self.up1 = Up(1024, 512 // factor, bilinear)
        self.up2 = Up(512, 256 // factor, bilinear)
        self.up3 = Up(256, 128 // factor, bilinear)
        self.up4 = Up(128, 64, bilinear)
        self.outc = OutConv(64, n_classes)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits
```

```python
import argparse
import logging
import os
import sys

import numpy as np
import torch
import torch.nn as nn
from torch import optim
from tqdm import tqdm

from eval import eval_net
from unet import UNet
from tensorboardX import SummaryWriter
from utils.dataset import BasicDataset
from torch.utils.data import DataLoader, random_split

dir_img = '/home/qq/code/Pytorch-UNet/cell_data/patch_data/train/image/'
dir_mask = '/home/qq/code/Pytorch-UNet/cell_data/patch_data/train/mask/'
dir_checkpoint = '/home/qq/code/Pytorch-UNet/checkpoints/'

def train_net(net,
              device,
              epochs=10,
              batch_size=20,
              lr=0.001,
              val_percent=0.1,
              save_cp=True,
              img_scale=0.5):

    dataset = BasicDataset(dir_img, dir_mask, img_scale)
    n_val = int(len(dataset) * val_percent)
    n_train = len(dataset) - n_val
    train, val = random_split(dataset, [n_train, n_val])
    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)

    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')
    global_step = 0

    logging.info(f'''Starting training:
        Epochs:          {epochs}
        Batch size:      {batch_size}
        Learning rate:   {lr}
        Training size:   {n_train}
        Validation size: {n_val}
        Checkpoints:     {save_cp}
        Device:          {device.type}
        Images scaling:  {img_scale}
    ''')

    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)
    if net.n_classes > 1:
        criterion = nn.CrossEntropyLoss()
    else:
        criterion = nn.BCEWithLogitsLoss()
    # define the best loss. use to save the best unet work arguments
    best_loss = 1000
    for epoch in range(epochs):
        logging.info("the best_loss is %f",best_loss)
        net.train()
        
        epoch_loss = 0
        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:
            for batch in train_loader:
                imgs = batch['image']
                true_masks = batch['mask']
                assert imgs.shape[1] == net.n_channels, \
                    f'Network has been defined with {net.n_channels} input channels, ' \
                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \
                    'the images are loaded correctly.'

                imgs = imgs.to(device=device, dtype=torch.float32)
                mask_type = torch.float32 if net.n_classes == 1 else torch.long
                true_masks = true_masks.to(device=device, dtype=mask_type)

                masks_pred = net(imgs)
                
                loss = criterion(masks_pred, true_masks)
                # should save the best result. But not do this.
                epoch_loss += loss.item()
                writer.add_scalar('Loss/train', loss.item(), global_step)

                pbar.set_postfix(**{'loss (batch)': loss.item()})

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_value_(net.parameters(), 0.1)
                optimizer.step()

                pbar.update(imgs.shape[0])
                global_step += 1
                if global_step % (n_train // (10 * batch_size)) == 0:
                    for tag, value in net.named_parameters():
                        tag = tag.replace('.', '/')
                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)
                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)
                    val_score = eval_net(net, val_loader, device)
                    scheduler.step(val_score)
                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)

                    if net.n_classes > 1:
                        logging.info('Validation cross entropy: {}'.format(val_score))
                        writer.add_scalar('Loss/test', val_score, global_step)
                    else:
                        logging.info('Validation Dice Coeff: {}'.format(val_score))
                        writer.add_scalar('Dice/test', val_score, global_step)

                    writer.add_images('images', imgs, global_step)
                    if net.n_classes == 1:
                        writer.add_images('masks/true', true_masks, global_step)
                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)

        if save_cp:
            try:
                os.mkdir(dir_checkpoint)
                logging.info('Created checkpoint directory')
            except OSError:
                pass
            if best_loss > epoch_loss:
                best_loss = epoch_loss
                torch.save(net.state_dict(),dir_checkpoint + f'Model.pth')
                logging.info(f'the best Checkpoint saved !')
                logging.info("the best_loss is %f",best_loss)
    writer.close()


def get_args():
    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=300,
                        help='Number of epochs', dest='epochs')
    parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=1,
                        help='Batch size', dest='batchsize')
    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.0001,
                        help='Learning rate', dest='lr')
    parser.add_argument('-f', '--load', dest='load', type=str, default=False,
                        help='Load model from a .pth file')
    parser.add_argument('-s', '--scale', dest='scale', type=float, default=0.5,
                        help='Downscaling factor of the images')
    parser.add_argument('-v', '--validation', dest='val', type=float, default=10.0,
                        help='Percent of the data that is used as validation (0-100)')
    return parser.parse_args()


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    args = get_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f'Using device {device}')

    # Change here to adapt to your data
    # n_channels=3 for RGB images
    # n_classes is the number of probabilities you want to get per pixel
    #   - For 1 class and background, use n_classes=1
    #   - For 2 classes, use n_classes=1
    #   - For N > 2 classes, use n_classes=N
    net = UNet(n_channels=3, n_classes=1, bilinear=True)
    logging.info(f'Network:\n'
                 f'\t{net.n_channels} input channels\n'
                 f'\t{net.n_classes} output channels (classes)\n'
                 f'\t{"Bilinear" if net.bilinear else "Transposed conv"} upscaling')
    print(net.n_classes)
    if args.load:
        net.load_state_dict(
            torch.load(args.load, map_location=device)
        )
        logging.info(f'Model loaded from {args.load}')

    net.to(device=device)
    # faster convolutions, but more memory
    # cudnn.benchmark = True

    try:
        train_net(net=net,
                  epochs=args.epochs,
                  batch_size=args.batchsize,
                  lr=args.lr,
                  device=device,
                  img_scale=args.scale,
                  val_percent=args.val / 100)
    except KeyboardInterrupt:
        torch.save(net.state_dict(), 'INTERRUPTED.pth')
        logging.info('Saved interrupt')
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
```





# 参数调整层面

# 训练技巧层面

