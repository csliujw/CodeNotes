# 基本概念

## 目的

看懂图片的内容。【视频可以提取关键帧的图片，然后进行识别】

## 原理

构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类。

## 挑战

### 特征难以提取

同一只猫在不同的角度，不同的光线，不同的动作下。像素差异是非常大的。就算是同一张照片，旋转90度后，其像素差异也非常大！

### 计算量大

手机上随便拍一张照片就是1000*2000像素的。每个像素 RGB 3个参数，一共有1000 X 2000 X 3=6,000,000。随便一张照片就要处理 600万 个参数

### 银弹 -- CNN

CNN很好的解决了上面的难点。

- CNN 可以有效的提取图像里的特征

- CNN 可以将海量的数据（不影响特征提取的前提下）进行有效的降维，大大减少了对算力的要求

## 任务

- 图像分类【人类识别，物体分类】
- **目标检测**【给定图片或视频帧 计算出目标所在的位置，并给出每个目标的具体类别】
- **语义分割**【将图像分成像素组，对像素组进行标记和分类。语义分割试图在语义上理解图像中每个像素是什么，除了识别出类别外，还需要确定物体的边界】
- 实例分割【将不同类型的实例进行分类，比如用 5 种不同颜色来标记 5 辆汽车。我们会看到多个重叠物体和不同背景的复杂景象，我们不仅需要将这些不同的对象进行分类，而且还要确定对象的边界、差异和彼此之间的关系！】
- 视频分类
- 人体关键点检测【通过人体关键节点的组合和追踪来识别人的运动和行为，对于描述人体姿态，预测人体行为至关重要】
- 场景文字识别
- 目标跟踪

# 语义分割

# FCN

CNN是卷积后有几个全连接层。FCN是都是卷积层（NN多数的参数都集中在了全连接层，全都是卷积层减少了参数的数量？）。FCN每做一次卷积和下采样（池化），图片大小都减少为上一次的一半。

**反卷积的意思**

<img src="https://img-blog.csdn.net/20161021141659634">

如图：下方的四个蓝色小块是我们需要反卷积的对象。我们要把这四块反卷积成上面的16块。

原理很简单，我们对这四块进行padding填充，填充成6 X 6的，再用3 x 3的卷积核进行卷积就可以还原了。而且，这四块是很多块的浓缩，所以是可以还原信息的。但是边缘信息是无法完美还原的。

- conv1  卷积+池化  变为原始图的 1/2
- conv2  卷积+池化  变为原始图的 1/4
- conv3 卷积+池化  变为原始图的 1/8     保留pool3的featureMap
- conv4  卷积+池化  变为原始图的 1/16  保留pool4的featureMap
- conv5  卷积+池化  变为原始图的 1/32 【然后把原来CNN操作中的全连接变成卷积操作conv6、conv7，图像的featureMap数量改变但是图像大小依然为原图的1/32，此时图像不再叫featureMap而是叫heatMap。】
- conv6  卷积+池化  变为原始图的 1/32
- conv7  卷积+池化  变为原始图的 1/32

现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个差值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。

<span style="color:red">作者测试过  ：将pool1， pool2的输出再上采样输出。不过，这样得到的结果提升并不大。</span>

**FCN的代码模版**

个人感觉他和论文的有出入，我们仅参考他上采样的代码部分！建议看caffe的代码结构  感觉好一些。

```python
'''
[1] Long, J., Shelhamer, E., & Darrell, T. (2015). 
    Fully convolutional networks for semantic segmentation. 
    In Proceedings of the IEEE CVPR (pp. 3431-3440).
'''
# PyTorch 版本
import torch.nn as nn
import torch.nn.functional as F


class FCNN(nn.Module):

    def __init__(self):
        super(FCNN, self).__init__()
        # Learnable layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
        nn.init.kaiming_normal(self.conv1.weight)
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        nn.init.kaiming_normal(self.conv2.weight)
        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)
        nn.init.kaiming_normal(self.conv3.weight)
        self.conv4 = nn.Conv2d(in_channels=16, out_channels=2, kernel_size=5, padding=2)
        nn.init.kaiming_normal(self.conv4.weight)
        self.upsample_mode = 'nearest'

    # to experiment with other upsampling techniques
    def set_upsample_mode(self, upsample_mode='nearest'):
        if (upsample_mode in ['nearest', 'linear', 'bilinear', 'trilinear']):
            self.upsample_mode = upsample_mode

    def forward(self, x):
        # x.size() = (N, 3, W, W) 
        x = F.relu(self.conv1(x))
        # x.size() = (N, 16, W, W) 
        x = F.relu(self.conv2(x))
        # x.size() = (N, 32, W, W) 
        x = F.max_pool2d(x, (2, 2))
        # x.size() = (N, 32, W/2, W/2)
        x = F.relu(self.conv3(x))
        # x.size() = (N, 16, W/2, W/2)
        x = F.upsample(x, scale_factor=2, mode=self.upsample_mode)
        # x.size() = (N, 16, W, W)
        x = self.conv4(x)
        # x.size() = (N, 2, W, W)
        return x
```

```python
import caffe
from caffe import layers as L, params as P
from caffe.coord_map import crop

def conv_relu(bottom, nout, ks=3, stride=1, pad=1):
    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,
        num_output=nout, pad=pad,
        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])
    return conv, L.ReLU(conv, in_place=True)

def max_pool(bottom, ks=2, stride=2):
    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)

def fcn(split):
    n = caffe.NetSpec()
    n.data, n.sem, n.geo = L.Python(module='siftflow_layers',
            layer='SIFTFlowSegDataLayer', ntop=3,
            param_str=str(dict(siftflow_dir='../data/sift-flow',
                split=split, seed=1337)))

    # the base net
    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)
    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)
    n.pool1 = max_pool(n.relu1_2)

    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)
    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)
    n.pool2 = max_pool(n.relu2_2)

    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)
    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)
    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)
    n.pool3 = max_pool(n.relu3_3)

    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)
    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)
    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)
    n.pool4 = max_pool(n.relu4_3)

    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)
    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)
    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)
    n.pool5 = max_pool(n.relu5_3)

    # fully conv
    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)
    # 正则化，丢弃50%的参数 让网络可以更快收敛？
    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)
    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)
    # 正则化，丢弃50%的参数 让网络可以更快收敛？
    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)

    n.score_fr_sem = L.Convolution(n.drop7, num_output=33, kernel_size=1, pad=0,
        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])
    n.upscore_sem = L.Deconvolution(n.score_fr_sem,
        convolution_param=dict(num_output=33, kernel_size=64, stride=32,
            bias_term=False),
        param=[dict(lr_mult=0)])
    n.score_sem = crop(n.upscore_sem, n.data)
    # loss to make score happy (o.w. loss_sem)
    n.loss = L.SoftmaxWithLoss(n.score_sem, n.sem,
            loss_param=dict(normalize=False, ignore_label=255))

    n.score_fr_geo = L.Convolution(n.drop7, num_output=3, kernel_size=1, pad=0,
        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])
    n.upscore_geo = L.Deconvolution(n.score_fr_geo,
        convolution_param=dict(num_output=3, kernel_size=64, stride=32,
            bias_term=False),
        param=[dict(lr_mult=0)])
    n.score_geo = crop(n.upscore_geo, n.data)
    n.loss_geo = L.SoftmaxWithLoss(n.score_geo, n.geo,
            loss_param=dict(normalize=False, ignore_label=255))

    return n.to_proto()

def make_net():
    with open('trainval.prototxt', 'w') as f:
        f.write(str(fcn('trainval')))

    with open('test.prototxt', 'w') as f:
        f.write(str(fcn('test')))

if __name__ == '__main__':
    make_net()
```

**总结一下（复制）**

CNN网络在卷积后是使用若干全连接层，将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。以`AlexNet`为代表的经典CNN结构适合于图像级的分类和回归任务，因为它们最后都期望得到整个输入图像的一个数值描述（概率），比如`AlexNet`的`ImageNet`模型输出一个1000维的向量表示输入图像属于每一类的概率(`softmax`归一化)。

FCN是对图像进行像素级的分类，解决了语义级别的图像分割问题。FCN采用放卷积层对最后一个卷积层的feature map进行上采样，恢复图片原来的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。

FCN与CNN的区别在把于CNN最后的全连接层换成卷积层

<a href="https://blog.csdn.net/sssssyuan/article/details/104321235">参考博客</a>

<a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">论文原文</a>

------

# FCN复现

FCN 语义分割的鼻祖！

## 语义

图片表达的意思 是图片的语义。像素表达的意思是像素的语义。

### 概念辨析

- 语义分割
- 实例分割： 只关注我关系的东西，不需要把背景都分割出来。
- 全景风格： 全景风格

----

# U-Net