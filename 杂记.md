# JVM内存可见性

[推荐阅读该博客](https://www.cnblogs.com/lqlqlq/p/13693876.html)

[推荐该博主](https://www.cnblogs.com/lqlqlq/)

可见性相关的源码在这个目录下

`orderAccess.hpp`

Ordeing a load relative to preceding stores requires a store_fence, which implies a member #StoreLoad

 between the store and load under sparc-TSO. A fence is required by ia64. On x86, we use locked xchg.

TSO guarantees that the hardware orders loads wrt  loads and stores, and stores wrt to each other.

TSO模型保证了硬件顺序，wrt好像是write

```CPP
// Consider the implementation of loadload. If your platform isn't

// cache-coherent, then loadload must not only prevent hardware load

// instruction reordering, but it must also ensure that subsequent

// loads from addresses that could be written by other processors (i.e.,

// that are broadcast by other processors) go all the way to the first

// level of memory shared by those processors and the one issuing

// the loadload.

inline void OrderAccess::loadload()   { acquire(); }
inline void OrderAccess::storestore() { release(); }
inline void OrderAccess::loadstore()  { acquire(); }
// store load 需要内存屏障
inline void OrderAccess::storeload()  { fence(); }

inline void OrderAccess::acquire() {
  volatile intptr_t local_dummy;
// 内存屏障的实现
#ifdef AMD64
  __asm__ volatile ("movq 0(%%rsp), %0" : "=r" (local_dummy) : : "memory");
#else
  __asm__ volatile ("movl 0(%%esp),%0" : "=r" (local_dummy) : : "memory");
#endif // AMD64
}
```

看源码发现 storeload操作x86一定会进行重排序操作，所以用指令屏障避免他重排序（fence）。

内存模型

- Java volatile
  - 内存顺序：cpu指令顺序；编译器优化指令顺序
  - 可见性
- C Volatile
  - 编译器不会优化，不做指令缓存，即不会把指令放寄存器里。

C语言的Volatile是单核处理器的时候出现的，为了解决单核处理器对于IO端口处理的问题



不是有MSEI了吗？为什么还要volatile？

一致性缓存协议好像是通过锁内存，锁总线的方式来实现的。

# Java锁

JOL查看Java对象布局

所谓的给对象上锁就是在对对象头里加了点信息

CAS是乐观锁 自选锁 无锁（无悲观锁）的一种实现。可以粗略的认为他们是通用的概念。

偏向锁是一个标记。

只有一个人的多线程时，会标记为偏向锁状态，再来人，擦除偏向锁升级为自选锁。但是JDK15把偏向锁废了。

Spring IOC AOP 源码debug看一次， 事务控制再刷一次视频！！！

# Python补充

```python
"""
python部分代码学习
"""

# 进度条
from collections import OrderedDict


def par():
    from tqdm import tqdm
    from time import sleep

    for i in tqdm(range(100)):
        sleep(0.1)


def par2():
    from tqdm import tqdm
    from time import sleep
    pars = tqdm([1, 2, 3, 4, 5, 6, 7, 8])
    pars.set_description_str("process")
    for i in pars:
        sleep(1)
        pars.set_description_str("process", i)


def par3():
    from tqdm import tqdm
    from time import sleep
    pars = tqdm([1, 2, 3, 4, 5, 6, 7, 8])
    for i in pars:
        sleep(0.3)
        # 设置好前缀
        day = OrderedDict([('loss', 1), ('iou', 1), ])
        pars.set_postfix(day)
    pars.close()


"""
pytorch 不进行梯度计算（反向传播？）
>>> x = torch.tensor([1], requires_grad=True)
>>> with torch.no_grad():
...   y = x * 2
>>> y.requires_grad
False
>>> @torch.no_grad()
... def doubler(x):
...     return x * 2
>>> z = doubler(x)
>>> z.requires_grad
False
"""

def no_grad():
    import torch
    x = torch.tensor([1.], requires_grad=True)
    y = 0
    with torch.no_grad():
        y = x * 2
    # False 不进行梯度计算
    print(y.requires_grad)

def have_grad():
    import torch
    x = torch.tensor([1.], requires_grad=True)
    y = 0
    y = x * 2
    # True
    print(y.requires_grad)

"""
命令行参数解析
"""
import argparse

def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--name', default=None,
                        help='model name: (default: arch+timestamp)')
    parser.add_argument('--epochs', default=100, type=int, metavar='N',
                        help='number of total epochs to run')
    parser.add_argument('-b', '--batch_size', default=8, type=int,
                        metavar='N', help='mini-batch size (default: 16)')

    parser.add_argument('--input_channels', default=3, type=int,
                        help='input channels')

    config = parser.parse_args()

    return config

# vars 把对象转成字典
config = vars(parse_args())
for key, value in config.items():
    print(key, value)
```